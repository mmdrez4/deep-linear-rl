{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W3Gq3o_bGy10",
        "ArzuUc8WFVOs",
        "joqj9DFyFkcq",
        "6LaO9SJLFo7p",
        "xH3J2_azF2Xh",
        "Wq2snh0iFuqi",
        "fek2zIOuF5Me",
        "8sYE30Ry5bsn"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Restart the session after running this cell (Run it once!)\n",
        "\n",
        "!git clone https://github.com/sparisi/gym_gridworlds\n",
        "\n",
        "!pip install ./gym_gridworlds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlY9Ig2pE5qj",
        "outputId": "fa147308-43e7-4a95-ba5c-539b92fbe017",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:51:21.911464Z",
          "iopub.execute_input": "2024-11-21T21:51:21.911743Z",
          "iopub.status.idle": "2024-11-21T21:51:35.920812Z",
          "shell.execute_reply.started": "2024-11-21T21:51:21.911717Z",
          "shell.execute_reply": "2024-11-21T21:51:35.919925Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gym_gridworlds'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 166 (delta 22), reused 17 (delta 17), pack-reused 139 (from 1)\u001b[K\n",
            "Receiving objects: 100% (166/166), 68.22 KiB | 6.82 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "Processing ./gym_gridworlds\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gymnasium (from Gym-Gridworlds==1.0)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from Gym-Gridworlds==1.0) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium->Gym-Gridworlds==1.0)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: Gym-Gridworlds\n",
            "  Building wheel for Gym-Gridworlds (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Gym-Gridworlds: filename=Gym_Gridworlds-1.0-py3-none-any.whl size=15432 sha256=3088ff529c94bffc30a4a0b61657f77f4733ed4d50710e5412068c2670df1a96\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-il2oaym2/wheels/35/b6/53/d1fcfbb39d514897bf6fbae43c1b74a9f3c71ff4f92c6d32d3\n",
            "Successfully built Gym-Gridworlds\n",
            "Installing collected packages: farama-notifications, gymnasium, Gym-Gridworlds\n",
            "Successfully installed Gym-Gridworlds-1.0 farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ArzuUc8WFVOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import gym\n",
        "\n",
        "import gymnasium\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json"
      ],
      "metadata": {
        "id": "HaIz8uDLTyZY",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:51:35.923127Z",
          "iopub.execute_input": "2024-11-21T21:51:35.923565Z",
          "iopub.status.idle": "2024-11-21T21:51:39.739468Z",
          "shell.execute_reply.started": "2024-11-21T21:51:35.923495Z",
          "shell.execute_reply": "2024-11-21T21:51:39.738567Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS8cWkmWGi4K",
        "outputId": "f48dbaa2-3fd9-4b27-e09f-5bc28ebd63b4",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:51:39.740428Z",
          "iopub.execute_input": "2024-11-21T21:51:39.740775Z",
          "iopub.status.idle": "2024-11-21T21:51:39.800963Z",
          "shell.execute_reply.started": "2024-11-21T21:51:39.740749Z",
          "shell.execute_reply": "2024-11-21T21:51:39.799990Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Kbdag6O4WFIj",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:51:39.802948Z",
          "iopub.execute_input": "2024-11-21T21:51:39.803272Z",
          "iopub.status.idle": "2024-11-21T21:51:39.808949Z",
          "shell.execute_reply.started": "2024-11-21T21:51:39.803227Z",
          "shell.execute_reply": "2024-11-21T21:51:39.807986Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "6LaO9SJLFo7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, depth, fc_unit, final_layer_unit):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        self.feature_size = 128\n",
        "\n",
        "        self.feature_layer = nn.Linear(state_size, self.feature_size)\n",
        "\n",
        "        self.init_weights(self.feature_layer)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(depth):\n",
        "\n",
        "            if depth == 1:\n",
        "\n",
        "                self.layers.append(nn.Linear(self.feature_size, final_layer_unit))\n",
        "\n",
        "            elif i == 0:\n",
        "\n",
        "                self.layers.append(nn.Linear(self.feature_size, fc_unit))\n",
        "\n",
        "            elif i == depth - 1:\n",
        "\n",
        "                self.layers.append(nn.Linear(fc_unit, final_layer_unit))\n",
        "\n",
        "            else:\n",
        "\n",
        "                self.layers.append(nn.Linear(fc_unit, fc_unit))\n",
        "\n",
        "\n",
        "\n",
        "        for layer in self.layers:\n",
        "\n",
        "          self.init_weights(layer)\n",
        "\n",
        "\n",
        "\n",
        "        if depth > 0:\n",
        "\n",
        "          self.final_layer = nn.Linear(final_layer_unit, action_size)\n",
        "\n",
        "        else:\n",
        "\n",
        "          self.final_layer = nn.Linear(self.feature_size, action_size)\n",
        "\n",
        "\n",
        "\n",
        "        #nn.init.zeros_(self.final_layer.weight)\n",
        "\n",
        "        #nn.init.zeros_(self.final_layer.bias)\n",
        "\n",
        "        self.init_weights(self.final_layer)\n",
        "\n",
        "\n",
        "\n",
        "    def init_weights(self, layer):\n",
        "\n",
        "        nn.init.xavier_normal(layer.weight)\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        x = self.feature_layer(state)\n",
        "        x = F.relu(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.final_layer(x)\n"
      ],
      "metadata": {
        "id": "k6kdTlQtHlmF",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:53:49.762272Z",
          "iopub.execute_input": "2024-11-21T21:53:49.762657Z",
          "iopub.status.idle": "2024-11-21T21:53:49.776312Z",
          "shell.execute_reply.started": "2024-11-21T21:53:49.762626Z",
          "shell.execute_reply": "2024-11-21T21:53:49.775270Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReplayBuffer"
      ],
      "metadata": {
        "id": "xH3J2_azF2Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "\n",
        "        self.memory.append(e)\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "WUEjnATTF0PM",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:51:39.833670Z",
          "iopub.execute_input": "2024-11-21T21:51:39.833978Z",
          "iopub.status.idle": "2024-11-21T21:51:39.846064Z",
          "shell.execute_reply.started": "2024-11-21T21:51:39.833940Z",
          "shell.execute_reply": "2024-11-21T21:51:39.845284Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "Wq2snh0iFuqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DQN agent class\n",
        "\n",
        "class DQNAgent:\n",
        "\n",
        "    # Initialize the DQN agent\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, lr, network):\n",
        "\n",
        "        self.state_size = state_size\n",
        "\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n",
        "\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
        "\n",
        "\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
        "\n",
        "        self.t_step = 0\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "\n",
        "        if self.t_step == 0:\n",
        "\n",
        "            if len(self.memory) > 64:\n",
        "\n",
        "                experiences = self.memory.sample()\n",
        "\n",
        "                self.learn(experiences, gamma=0.99)\n",
        "\n",
        "\n",
        "\n",
        "    # Choose an action based on the current state\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        #print(state)\n",
        "\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        self.qnetwork_local.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            action_values = self.qnetwork_local(state_tensor)\n",
        "\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "\n",
        "\n",
        "        if np.random.random() > eps:\n",
        "\n",
        "            return action_values.argmax(dim=1).item()\n",
        "\n",
        "        else:\n",
        "\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "\n",
        "\n",
        "    # Learn from batch of experiences\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
        "\n",
        "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
        "\n",
        "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
        "\n",
        "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
        "\n",
        "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
        "\n",
        "\n",
        "\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "SMqPt_ruIG59",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:51:39.847136Z",
          "iopub.execute_input": "2024-11-21T21:51:39.847427Z",
          "iopub.status.idle": "2024-11-21T21:51:39.861502Z",
          "shell.execute_reply.started": "2024-11-21T21:51:39.847390Z",
          "shell.execute_reply": "2024-11-21T21:51:39.860720Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "WxOD0rGTF_Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 500\n",
        "\n",
        "max_steps_per_episode_train = 150\n",
        "\n",
        "#max_steps_per_episode_eval = 25\n",
        "\n",
        "epsilon_start = 1.0\n",
        "\n",
        "epsilon_end = 0.01\n",
        "\n",
        "epsilon_decay_rate = 0.99\n",
        "\n",
        "gamma = 0.99\n",
        "\n",
        "lr = 1e-4\n",
        "\n",
        "buffer_size = 10000\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "update_frequency = 3\n",
        "\n",
        "test_episodes = 1"
      ],
      "metadata": {
        "id": "ztLArg8OF-4o",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:56:51.024143Z",
          "iopub.execute_input": "2024-11-21T21:56:51.024959Z",
          "iopub.status.idle": "2024-11-21T21:56:51.029247Z",
          "shell.execute_reply.started": "2024-11-21T21:56:51.024925Z",
          "shell.execute_reply": "2024-11-21T21:56:51.028402Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Environment"
      ],
      "metadata": {
        "id": "fek2zIOuF5Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#env_id = \"Gym-Gridworlds/DangerMaze-6x6-v0\"\n",
        "\n",
        "#env_id = \"Gym-Gridworlds/Empty-10x10-v0\"\n",
        "\n",
        "#env_id = \"Gym-Gridworlds/Penalty-3x3-v0\"\n",
        "\n",
        "#env = gymnasium.make(env_id, render_mode=\"binary\")\n",
        "\n",
        "#env_eval = gymnasium.make(env_id, max_episode_steps=max_steps_per_episode_eval, render_mode=\"binary\")\n",
        "\n",
        "env = gym.make('Acrobot-v1')\n",
        "\n",
        "env_eval = gym.make('Acrobot-v1')"
      ],
      "metadata": {
        "id": "nSD8eiX5IG5-",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:51:39.878453Z",
          "iopub.execute_input": "2024-11-21T21:51:39.878725Z",
          "iopub.status.idle": "2024-11-21T21:51:39.894872Z",
          "shell.execute_reply.started": "2024-11-21T21:51:39.878702Z",
          "shell.execute_reply": "2024-11-21T21:51:39.894012Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Agent"
      ],
      "metadata": {
        "id": "R_nWh5QTGSUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = env.observation_space.shape[0]\n",
        "\n",
        "output_dim = env.action_space.n\n",
        "\n",
        "#depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50]\n",
        "\n",
        "#fc_units = [500, 366, 262, 215, 187, 168, 153, 142, 133, 125, 87, 54]\n",
        "\n",
        "#final_units = [500, 366, 263, 217, 188, 165, 157, 144, 134, 135, 75, 54]\n",
        "\n",
        "depths = [1]\n",
        "\n",
        "fc_units = [1024]\n",
        "\n",
        "final_units = [1024]\n",
        "\n",
        "networks = [QNetwork(input_dim, output_dim, seed=i, depth=depths[i], fc_unit=fc_units[i], final_layer_unit=final_units[i]) for i in range(len(depths))]\n",
        "\n",
        "netwrok_names = [f\"DQN_{depths[i]:02}_{fc_units[i]}_{sum(p.numel() for p in networks[i].parameters())}\" for i in range(len(depths))]\n",
        "\n",
        "data = {}\n",
        "\n",
        "for network in networks:\n",
        "\n",
        "  data[network] = {\"loss\": [], \"train_reward\": [], \"eval_reward\": []}\n",
        "\n",
        "\n",
        "\n",
        "netwrok_names"
      ],
      "metadata": {
        "id": "H2pzwB0CRL7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00f0e8d-7eba-4e65-d870-45288055a881",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:54:18.089990Z",
          "iopub.execute_input": "2024-11-21T21:54:18.090287Z",
          "iopub.status.idle": "2024-11-21T21:54:18.332376Z",
          "shell.execute_reply.started": "2024-11-21T21:54:18.090261Z",
          "shell.execute_reply": "2024-11-21T21:54:18.331563Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DQN_01_1024_107199']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "8sYE30Ry5bsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(new_agent):\n",
        "\n",
        "  episode_rewards = []\n",
        "\n",
        "\n",
        "\n",
        "  action_series = []\n",
        "\n",
        "  for episode in range(test_episodes):\n",
        "\n",
        "      action_series.append([])\n",
        "\n",
        "      state = env_eval.reset()\n",
        "\n",
        "      episode_reward = 0\n",
        "\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "\n",
        "          action = new_agent.act(state, eps=0.)\n",
        "\n",
        "          next_state, reward, done, _ = env_eval.step(action)\n",
        "          #done = term or trunc\n",
        "\n",
        "          action_series[-1].append(action)\n",
        "\n",
        "          episode_reward += reward\n",
        "\n",
        "          state = next_state\n",
        "\n",
        "\n",
        "\n",
        "      episode_rewards.append(episode_reward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  average_reward = sum(episode_rewards) / test_episodes\n",
        "\n",
        "  return average_reward, action_series"
      ],
      "metadata": {
        "id": "JdLpXclb5dae",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:55:42.074732Z",
          "iopub.execute_input": "2024-11-21T21:55:42.075197Z",
          "iopub.status.idle": "2024-11-21T21:55:42.084119Z",
          "shell.execute_reply.started": "2024-11-21T21:55:42.075153Z",
          "shell.execute_reply": "2024-11-21T21:55:42.083243Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "VEyTwl3aGWTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in range(1):\n",
        "\n",
        "  print(f\"seed = {seed}\")\n",
        "\n",
        "  for i, network in enumerate(networks):\n",
        "\n",
        "    if i not in [0]:\n",
        "\n",
        "      continue\n",
        "\n",
        "\n",
        "\n",
        "    buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "\n",
        "\n",
        "    new_agent = DQNAgent(input_dim, output_dim, seed=seed, lr = lr, network={\"depth\": depths[i], \"fc_units\": fc_units[i], \"final_fc_units\": final_units[i]})\n",
        "\n",
        "\n",
        "\n",
        "    # Training\n",
        "\n",
        "\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    average_rewards = []\n",
        "\n",
        "\n",
        "\n",
        "    ep_mean_reward = 0\n",
        "\n",
        "    ep_mean_reward_fixed = 0\n",
        "\n",
        "    ep_mean_loss = 0\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "\n",
        "\n",
        "    pbar = trange(num_episodes, position=0, leave=True, desc=f\"{netwrok_names[i]}\", unit=\"episode\")\n",
        "\n",
        "\n",
        "\n",
        "    for episode in pbar:\n",
        "\n",
        "        # Reset the environment\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay_rate)\n",
        "\n",
        "        ep_reward = 0\n",
        "\n",
        "        ep_loss = 0\n",
        "\n",
        "        #print(state)\n",
        "\n",
        "        # Run one episode\n",
        "\n",
        "        for step in range(max_steps_per_episode_train):\n",
        "\n",
        "            # Choose and perform an action\n",
        "\n",
        "            action = new_agent.act(state, epsilon)\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            #done = term or trunc\n",
        "\n",
        "            #if reward > 0:\n",
        "\n",
        "            #  print(\"REWARDDDDDD!\")\n",
        "\n",
        "            buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "\n",
        "\n",
        "            #print(len(buffer), batch_size)\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "\n",
        "                batch = random.sample(buffer, batch_size)\n",
        "\n",
        "                # Update the agent's knowledge\n",
        "\n",
        "                loss = new_agent.learn(batch, gamma)\n",
        "\n",
        "                ep_loss += loss\n",
        "\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "\n",
        "            ep_reward += reward\n",
        "\n",
        "            # Check if the episode has ended\n",
        "\n",
        "            if done:\n",
        "\n",
        "              #print(\"H\")\n",
        "\n",
        "              break\n",
        "\n",
        "\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "\n",
        "        ep_mean_reward += ep_reward\n",
        "\n",
        "        if episode % update_frequency == 0:\n",
        "\n",
        "          ep_mean_reward_fixed = ep_mean_reward\n",
        "\n",
        "          average_reward, action_series = eval(new_agent)\n",
        "\n",
        "          ep_mean_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "        #if episode % (update_frequency * update_frequency) == 0:\n",
        "\n",
        "        #  eval_Q_plot(new_agent)\n",
        "\n",
        "          #print(action_series)\n",
        "\n",
        "\n",
        "\n",
        "        average_rewards.append(average_reward)\n",
        "\n",
        "\n",
        "\n",
        "        if type(ep_loss) == int:\n",
        "\n",
        "          record = 0\n",
        "\n",
        "          pbar.set_postfix(loss=ep_loss, ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n",
        "\n",
        "          losses.append(ep_loss)\n",
        "\n",
        "        else:\n",
        "\n",
        "          record = 1\n",
        "\n",
        "          pbar.set_postfix(loss=ep_loss.item(), ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n",
        "\n",
        "          losses.append(ep_loss.detach().cpu())\n",
        "\n",
        "\n",
        "\n",
        "    data[network][\"eval_reward\"].append(average_rewards)\n",
        "\n",
        "    if record == 1:\n",
        "\n",
        "      data[network][\"loss\"].append(losses)\n",
        "\n",
        "    data[network][\"train_reward\"].append(rewards)\n",
        "\n",
        "\n",
        "\n",
        "    to_be_saved = {\"loss\": np.array(losses).tolist(), \"train_reward\": rewards, \"eval_reward\": average_rewards}\n",
        "\n",
        "    with open(f\"/kaggle/working/Acrobat_{netwrok_names[i][:7]}{seed}.json\", \"w\") as f:\n",
        "\n",
        "      json.dump(to_be_saved, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "6rpzjfY2IG5-",
        "outputId": "52e26ec8-870c-4b9c-998d-70daf6cebf1f",
        "execution": {
          "iopub.status.busy": "2024-11-21T21:55:44.234349Z",
          "iopub.execute_input": "2024-11-21T21:55:44.234696Z",
          "iopub.status.idle": "2024-11-21T21:56:29.510717Z",
          "shell.execute_reply.started": "2024-11-21T21:55:44.234666Z",
          "shell.execute_reply": "2024-11-21T21:56:29.509566Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DQN_01_1024_107199: 100%|██████████| 500/500 [05:15<00:00,  1.58episode/s, ep_reward=-203, epsilon=0.01, loss=147, train_reward=-109]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/working/Acrobat_DQN_01_0.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-7a3883b5663e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mto_be_saved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_reward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval_reward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maverage_rewards\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/kaggle/working/Acrobat_{netwrok_names[i][:7]}{seed}.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_be_saved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/Acrobat_DQN_01_0.json'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "average_rewards"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OcFrpzce9G5",
        "outputId": "0938a2fb-ba69-4367-dc96-9b1c1b798076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -113.0,\n",
              " -113.0,\n",
              " -113.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -112.0,\n",
              " -112.0,\n",
              " -112.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -65.0,\n",
              " -65.0,\n",
              " -65.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -228.0,\n",
              " -228.0,\n",
              " -228.0,\n",
              " -251.0,\n",
              " -251.0,\n",
              " -251.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -193.0,\n",
              " -193.0,\n",
              " -193.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -116.0,\n",
              " -116.0,\n",
              " -116.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -72.0,\n",
              " -72.0,\n",
              " -72.0,\n",
              " -134.0,\n",
              " -134.0,\n",
              " -134.0,\n",
              " -137.0,\n",
              " -137.0,\n",
              " -137.0,\n",
              " -154.0,\n",
              " -154.0,\n",
              " -154.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -171.0,\n",
              " -171.0,\n",
              " -171.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -478.0,\n",
              " -478.0,\n",
              " -478.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -179.0,\n",
              " -179.0,\n",
              " -179.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -152.0,\n",
              " -152.0,\n",
              " -152.0,\n",
              " -161.0,\n",
              " -161.0,\n",
              " -161.0,\n",
              " -191.0,\n",
              " -191.0,\n",
              " -191.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -417.0,\n",
              " -417.0,\n",
              " -417.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -110.0,\n",
              " -110.0,\n",
              " -110.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -123.0,\n",
              " -123.0,\n",
              " -123.0,\n",
              " -366.0,\n",
              " -366.0,\n",
              " -366.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -228.0,\n",
              " -228.0,\n",
              " -228.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -132.0,\n",
              " -132.0,\n",
              " -132.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -179.0,\n",
              " -179.0,\n",
              " -179.0,\n",
              " -143.0,\n",
              " -143.0,\n",
              " -143.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -203.0,\n",
              " -203.0,\n",
              " -203.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -160.0,\n",
              " -160.0,\n",
              " -160.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -205.0,\n",
              " -205.0,\n",
              " -205.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -192.0,\n",
              " -192.0,\n",
              " -192.0,\n",
              " -82.0,\n",
              " -82.0,\n",
              " -82.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -121.0,\n",
              " -121.0,\n",
              " -121.0,\n",
              " -125.0,\n",
              " -125.0,\n",
              " -125.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -248.0,\n",
              " -248.0,\n",
              " -248.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -185.0,\n",
              " -185.0,\n",
              " -185.0,\n",
              " -198.0,\n",
              " -198.0,\n",
              " -198.0,\n",
              " -233.0,\n",
              " -233.0,\n",
              " -233.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -110.0,\n",
              " -110.0,\n",
              " -110.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -136.0,\n",
              " -136.0,\n",
              " -136.0,\n",
              " -255.0,\n",
              " -255.0,\n",
              " -255.0,\n",
              " -137.0,\n",
              " -137.0,\n",
              " -137.0,\n",
              " -109.0,\n",
              " -109.0,\n",
              " -109.0,\n",
              " -322.0,\n",
              " -322.0,\n",
              " -322.0,\n",
              " -137.0,\n",
              " -137.0,\n",
              " -137.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -94.0,\n",
              " -94.0,\n",
              " -94.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -144.0,\n",
              " -144.0,\n",
              " -144.0,\n",
              " -97.0,\n",
              " -97.0,\n",
              " -97.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -385.0,\n",
              " -385.0,\n",
              " -385.0,\n",
              " -395.0,\n",
              " -395.0,\n",
              " -395.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -141.0,\n",
              " -141.0,\n",
              " -141.0,\n",
              " -153.0,\n",
              " -153.0,\n",
              " -153.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -146.0,\n",
              " -146.0,\n",
              " -146.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -76.0,\n",
              " -76.0,\n",
              " -76.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -111.0,\n",
              " -111.0,\n",
              " -111.0,\n",
              " -115.0,\n",
              " -115.0,\n",
              " -115.0,\n",
              " -121.0,\n",
              " -121.0,\n",
              " -121.0,\n",
              " -91.0,\n",
              " -91.0,\n",
              " -91.0,\n",
              " -109.0,\n",
              " -109.0,\n",
              " -109.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -500.0,\n",
              " -203.0,\n",
              " -203.0]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "ptjV-i4LGaj4"
      }
    }
  ]
}