{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["W3Gq3o_bGy10","ArzuUc8WFVOs","joqj9DFyFkcq","6LaO9SJLFo7p","xH3J2_azF2Xh","Wq2snh0iFuqi","fek2zIOuF5Me","8sYE30Ry5bsn"],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Restart the session after running this cell (Run it once!)\n\n!git clone https://github.com/sparisi/gym_gridworlds\n\n!pip install ./gym_gridworlds","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlY9Ig2pE5qj","outputId":"f96c1f99-a966-48c2-e347-e9728ffe8d98","execution":{"iopub.status.busy":"2024-11-21T21:51:21.911464Z","iopub.execute_input":"2024-11-21T21:51:21.911743Z","iopub.status.idle":"2024-11-21T21:51:35.920812Z","shell.execute_reply.started":"2024-11-21T21:51:21.911717Z","shell.execute_reply":"2024-11-21T21:51:35.919925Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'gym_gridworlds'...\nremote: Enumerating objects: 166, done.\u001b[K\nremote: Counting objects: 100% (27/27), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 166 (delta 22), reused 17 (delta 17), pack-reused 139 (from 1)\u001b[K\nReceiving objects: 100% (166/166), 68.22 KiB | 1.48 MiB/s, done.\nResolving deltas: 100% (86/86), done.\nProcessing ./gym_gridworlds\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (from Gym-Gridworlds==1.0) (0.29.0)\nCollecting pygame (from Gym-Gridworlds==1.0)\n  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (3.0.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (0.0.4)\nDownloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: Gym-Gridworlds\n  Building wheel for Gym-Gridworlds (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for Gym-Gridworlds: filename=Gym_Gridworlds-1.0-py3-none-any.whl size=15432 sha256=f31d4ed0826c1a21522d2d2dd5f4b9f2d9748904548d2c680325e0ba0737c387\n  Stored in directory: /tmp/pip-ephem-wheel-cache-vea0tcgr/wheels/1b/52/46/c69490b45c86839705223517c2823ee0f3c6ed283b7b6a5e93\nSuccessfully built Gym-Gridworlds\nInstalling collected packages: pygame, Gym-Gridworlds\nSuccessfully installed Gym-Gridworlds-1.0 pygame-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"id":"ArzuUc8WFVOs"}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\n\nimport torch.nn as nn\n\ntorch._dynamo.config.suppress_errors = True\n\nimport torch.optim as optim\n\nimport torch.nn.functional as F\n\nimport random\n\nfrom collections import namedtuple, deque\n\nimport gym\n\nimport gymnasium\n\nfrom tqdm import trange\n\nimport matplotlib.pyplot as plt\n\nimport json","metadata":{"id":"HaIz8uDLTyZY","execution":{"iopub.status.busy":"2024-11-21T21:51:35.923127Z","iopub.execute_input":"2024-11-21T21:51:35.923565Z","iopub.status.idle":"2024-11-21T21:51:39.739468Z","shell.execute_reply.started":"2024-11-21T21:51:35.923495Z","shell.execute_reply":"2024-11-21T21:51:39.738567Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WS8cWkmWGi4K","outputId":"70b79463-7d1a-438e-ef53-ec7ed1b089c1","execution":{"iopub.status.busy":"2024-11-21T21:51:39.740428Z","iopub.execute_input":"2024-11-21T21:51:39.740775Z","iopub.status.idle":"2024-11-21T21:51:39.800963Z","shell.execute_reply.started":"2024-11-21T21:51:39.740749Z","shell.execute_reply":"2024-11-21T21:51:39.799990Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"id":"Kbdag6O4WFIj","execution":{"iopub.status.busy":"2024-11-21T21:51:39.802948Z","iopub.execute_input":"2024-11-21T21:51:39.803272Z","iopub.status.idle":"2024-11-21T21:51:39.808949Z","shell.execute_reply.started":"2024-11-21T21:51:39.803227Z","shell.execute_reply":"2024-11-21T21:51:39.807986Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Network","metadata":{"id":"6LaO9SJLFo7p"}},{"cell_type":"code","source":"def tile_features(\n\n    state,\n\n    centers,\n\n    widths,\n\n    offsets: list = [0]\n\n):\n\n    state = torch.tensor(state).float().to(device)\n\n    centers = torch.tensor(centers).float().to(device)\n\n    widths = torch.tensor(widths).float().to(device)\n\n\n\n    D = centers.shape[0]\n\n    N = state.shape[0]\n\n\n\n    #new_state = np.repeat(state[:, None, :], D, axis=1)\n\n    new_state = state[:, None, :].repeat(1, D, 1)\n\n\n\n    #widths = widths[None, None, :].repeat(N, D, 1)\n\n    #output = np.zeros((N, D))\n\n    output = torch.zeros((N, D), device=state.device)\n\n    for offset in offsets:\n\n        shifted_center = centers + offset\n\n        new_center = shifted_center[None, :, :].repeat(N, 1, 1)\n\n        #new_center = np.repeat(shifted_center[None, :, :], N, axis=0)\n\n        #print(new_center.shape, torch.norm(new_state - new_center, float('inf'), dim=2).shape, widths.shape)\n\n        output += (torch.norm(new_state - new_center, float('2'), dim=2) < widths).type(torch.float32)\n\n        #output += np.array(np.linalg.norm(new_state - new_center, np.inf, axis=2) < widths, dtype=np.float32)\n\n\n\n    return output / len(offsets)","metadata":{"id":"zhHTgT2-Hjwc","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T21:51:39.810042Z","iopub.execute_input":"2024-11-21T21:51:39.810336Z","iopub.status.idle":"2024-11-21T21:51:39.818170Z","shell.execute_reply.started":"2024-11-21T21:51:39.810311Z","shell.execute_reply":"2024-11-21T21:51:39.817438Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define the neural network model\n\nclass QNetwork(nn.Module):\n\n    def __init__(self, state_size, action_size, seed, depth, fc_unit, final_layer_unit):\n\n        super().__init__()\n\n        self.seed = torch.manual_seed(seed)\n\n        self.feature_size = self.feature_extract_init(state_size, action_size)\n\n\n\n        self.layers = nn.ModuleList()\n\n        for i in range(depth):\n\n            if depth == 1:\n\n                self.layers.append(nn.Linear(self.feature_size, final_layer_unit))\n\n            elif i == 0:\n\n                self.layers.append(nn.Linear(self.feature_size, fc_unit))\n\n            elif i == depth - 1:\n\n                self.layers.append(nn.Linear(fc_unit, final_layer_unit))\n\n            else:\n\n                self.layers.append(nn.Linear(fc_unit, fc_unit))\n\n\n\n        for layer in self.layers:\n\n          self.init_weights(layer)\n\n\n\n        if depth > 0:\n\n          self.final_layer = nn.Linear(final_layer_unit, action_size)\n\n        else:\n\n          self.final_layer = nn.Linear(self.feature_size, action_size)\n\n\n\n        #nn.init.zeros_(self.final_layer.weight)\n\n        #nn.init.zeros_(self.final_layer.bias)\n\n        self.init_weights(self.final_layer)\n\n\n\n    def init_weights(self, layer):\n\n        nn.init.xavier_normal(layer.weight)\n\n        self.to(device)\n\n\n\n    def feature_extract_init(self, state_size, action_size):\n\n        #low = torch.tensor(np.array([-4.8, -20, -4.8, -20]), device=device, dtype=torch.float32) # Convert to tensor and move to device\n\n        #high = torch.tensor(np.array([4.8, 20, 4.8, 20]), device=device, dtype=torch.float32)  # Convert to tensor and move to device\n\n        low = torch.tensor(env.observation_space.low, device=device, dtype=torch.float32)\n\n        high = torch.tensor(env.observation_space.high, device=device, dtype=torch.float32)\n\n        n_bins = 5\n\n        n_centers = np.repeat(n_bins, env.observation_space.shape[0])\n\n        offset_pos = np.linspace(-1, 1, 3 * n_bins)\n\n        #print(offset_pos)\n\n        centers = np.array(\n\n            np.meshgrid(*[\n\n                np.linspace(-1, 1, n_centers[i])\n\n                for i in range(env.observation_space.shape[0])\n\n            ])\n\n        ).reshape(env.observation_space.shape[0], -1).T\n\n        centers = torch.tensor(centers, device=device, dtype=torch.float32)  # Convert to tensor and move to device\n\n\n\n        self.feature_name, self.feature_extract = \"Tile\", lambda state: tile_features(((torch.tensor(state).to(device) - low) / (high - low)).reshape(-1, state_size), centers, 1.0, offset_pos)\n\n        return self.feature_extract(env.reset()[0]).shape[1]\n\n    # def feature_extract_init(self, state_size, action_size):\n\n    #     low = torch.tensor(np.array([-4.8, -10, -4.8, -10]))\n\n    #     high = torch.tensor(np.array([4.8, 10, 4.8, 10]))\n\n    #     n_bins = 10\n\n    #     n_centers = np.repeat(n_bins, env.observation_space.shape[0])\n\n    #     offset_pos = 2/(3*n_bins)\n\n    #     centers = np.array(\n\n    #       np.meshgrid(*[\n\n    #           np.linspace(-1, 1, n_centers[i])\n\n    #           for i in range(env.observation_space.shape[0])\n\n    #       ])\n\n    #     ).reshape(env.observation_space.shape[0], -1).T\n\n    #     centers = torch.tensor(centers).float().to(device)\n\n    #     #widths = 1.2 * (env.observation_space.high - env.observation_space.low)/(n_bins)\n\n    #     #print(centers.shape, state.shape)\n\n    #     #self.feature_name, self.feature_extract = \"Aggregate\", lambda state : aggregation_features(state.reshape(-1, state_size), centers)\n\n    #     #self.feature_name, self.feature_extract = \"RBF\", lambda state : rbf_features(state.reshape(-1, state_size), centers, 0.2)\n\n    #     self.feature_name, self.feature_extract = \"Tile\", lambda state : tile_features(((state - low) / (high - low)).reshape(-1, state_size), centers, 0.5)\n\n    #     #print(env.reset())\n\n    #     #print(self.feature_extract(env.reset()).shape)\n\n    #     return self.feature_extract(env.reset()).shape[1]\n\n\n\n\n\n    def forward(self, state):\n\n        #print(state.shape)\n\n        #print(state)\n\n        x = self.feature_extract(state)\n\n        #print(len(x[0]), torch.sum(x), torch.sum(x > 0))\n\n        #x = state\n\n        for layer in self.layers:\n\n            x = layer(x)\n\n        return self.final_layer(x)\n","metadata":{"id":"k6kdTlQtHlmF","execution":{"iopub.status.busy":"2024-11-21T21:53:49.762272Z","iopub.execute_input":"2024-11-21T21:53:49.762657Z","iopub.status.idle":"2024-11-21T21:53:49.776312Z","shell.execute_reply.started":"2024-11-21T21:53:49.762626Z","shell.execute_reply":"2024-11-21T21:53:49.775270Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# ReplayBuffer","metadata":{"id":"xH3J2_azF2Xh"}},{"cell_type":"code","source":"class ReplayBuffer:\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n\n        self.action_size = action_size\n\n        self.memory = deque(maxlen=buffer_size)\n\n        self.batch_size = batch_size\n\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n\n        self.seed = random.seed(seed)\n\n\n\n    def add(self, state, action, reward, next_state, done):\n\n        e = self.experience(state, action, reward, next_state, done)\n\n        self.memory.append(e)\n\n\n\n    def sample(self):\n\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n\n\n\n        return (states, actions, rewards, next_states, dones)\n\n\n\n    def __len__(self):\n\n        return len(self.memory)","metadata":{"id":"WUEjnATTF0PM","execution":{"iopub.status.busy":"2024-11-21T21:51:39.833670Z","iopub.execute_input":"2024-11-21T21:51:39.833978Z","iopub.status.idle":"2024-11-21T21:51:39.846064Z","shell.execute_reply.started":"2024-11-21T21:51:39.833940Z","shell.execute_reply":"2024-11-21T21:51:39.845284Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# DQN","metadata":{"id":"Wq2snh0iFuqi"}},{"cell_type":"code","source":"# Define the DQN agent class\n\nclass DQNAgent:\n\n    # Initialize the DQN agent\n\n    def __init__(self, state_size, action_size, seed, lr, network):\n\n        self.state_size = state_size\n\n        self.action_size = action_size\n\n        self.seed = random.seed(seed)\n\n\n\n        self.qnetwork_local = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n\n        self.qnetwork_target = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n\n\n\n        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n\n        self.t_step = 0\n\n\n\n    def step(self, state, action, reward, next_state, done):\n\n        self.memory.add(state, action, reward, next_state, done)\n\n\n\n        self.t_step = (self.t_step + 1) % 4\n\n        if self.t_step == 0:\n\n            if len(self.memory) > 64:\n\n                experiences = self.memory.sample()\n\n                self.learn(experiences, gamma=0.99)\n\n\n\n    # Choose an action based on the current state\n\n    def act(self, state, eps=0.):\n\n        #print(state)\n\n        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n\n        self.qnetwork_local.eval()\n\n        with torch.no_grad():\n\n            action_values = self.qnetwork_local(state_tensor)\n\n        self.qnetwork_local.train()\n\n\n\n        if np.random.random() > eps:\n\n            return action_values.argmax(dim=1).item()\n\n        else:\n\n            return np.random.randint(self.action_size)\n\n\n\n    # Learn from batch of experiences\n\n    def learn(self, experiences, gamma):\n\n        states, actions, rewards, next_states, dones = zip(*experiences)\n\n        states = torch.from_numpy(np.vstack(states)).float().to(device)\n\n        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n\n        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n\n        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n\n        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n\n\n\n        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n\n\n\n        Q_expected = self.qnetwork_local(states).gather(1, actions)\n\n\n\n        loss = F.mse_loss(Q_expected, Q_targets)\n\n        self.optimizer.zero_grad()\n\n        loss.backward()\n\n        self.optimizer.step()\n\n\n\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n\n\n\n        return loss\n\n\n\n    def soft_update(self, local_model, target_model, tau):\n\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n\n            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)","metadata":{"id":"SMqPt_ruIG59","execution":{"iopub.status.busy":"2024-11-21T21:51:39.847136Z","iopub.execute_input":"2024-11-21T21:51:39.847427Z","iopub.status.idle":"2024-11-21T21:51:39.861502Z","shell.execute_reply.started":"2024-11-21T21:51:39.847390Z","shell.execute_reply":"2024-11-21T21:51:39.860720Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{"id":"WxOD0rGTF_Ee"}},{"cell_type":"code","source":"num_episodes = 500\n\nmax_steps_per_episode_train = 150\n\n#max_steps_per_episode_eval = 25\n\nepsilon_start = 1.0\n\nepsilon_end = 0.01\n\nepsilon_decay_rate = 0.99\n\ngamma = 0.99\n\nlr = 1e-5\n\nbuffer_size = 10000\n\nbatch_size = 256\n\nupdate_frequency = 8\n\ntest_episodes = 1","metadata":{"id":"ztLArg8OF-4o","execution":{"iopub.status.busy":"2024-11-21T21:56:51.024143Z","iopub.execute_input":"2024-11-21T21:56:51.024959Z","iopub.status.idle":"2024-11-21T21:56:51.029247Z","shell.execute_reply.started":"2024-11-21T21:56:51.024925Z","shell.execute_reply":"2024-11-21T21:56:51.028402Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Setup Environment","metadata":{"id":"fek2zIOuF5Me"}},{"cell_type":"code","source":"#env_id = \"Gym-Gridworlds/DangerMaze-6x6-v0\"\n\n#env_id = \"Gym-Gridworlds/Empty-10x10-v0\"\n\n#env_id = \"Gym-Gridworlds/Penalty-3x3-v0\"\n\n#env = gymnasium.make(env_id, render_mode=\"binary\")\n\n#env_eval = gymnasium.make(env_id, max_episode_steps=max_steps_per_episode_eval, render_mode=\"binary\")\n\nenv = gym.make('Acrobot-v1')\n\nenv_eval = gym.make('Acrobot-v1')","metadata":{"id":"nSD8eiX5IG5-","execution":{"iopub.status.busy":"2024-11-21T21:51:39.878453Z","iopub.execute_input":"2024-11-21T21:51:39.878725Z","iopub.status.idle":"2024-11-21T21:51:39.894872Z","shell.execute_reply.started":"2024-11-21T21:51:39.878702Z","shell.execute_reply":"2024-11-21T21:51:39.894012Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# DQN Agent","metadata":{"id":"R_nWh5QTGSUN"}},{"cell_type":"code","source":"input_dim = env.observation_space.shape[0]\n\noutput_dim = env.action_space.n\n\n#depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50]\n\n#fc_units = [500, 366, 262, 215, 187, 168, 153, 142, 133, 125, 87, 54]\n\n#final_units = [500, 366, 263, 217, 188, 165, 157, 144, 134, 135, 75, 54]\n\ndepths = [1]\n\nfc_units = [256]\n\nfinal_units = [256]\n\nnetworks = [QNetwork(input_dim, output_dim, seed=i, depth=depths[i], fc_unit=fc_units[i], final_layer_unit=final_units[i]) for i in range(len(depths))]\n\nnetwrok_names = [f\"DQN_{depths[i]:02}_{fc_units[i]}_{sum(p.numel() for p in networks[i].parameters())}\" for i in range(len(depths))]\n\ndata = {}\n\nfor network in networks:\n\n  data[network] = {\"loss\": [], \"train_reward\": [], \"eval_reward\": []}\n\n\n\nnetwrok_names","metadata":{"id":"H2pzwB0CRL7X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cadb47da-dbef-4cd0-abe0-1bc036026b7b","execution":{"iopub.status.busy":"2024-11-21T21:54:18.089990Z","iopub.execute_input":"2024-11-21T21:54:18.090287Z","iopub.status.idle":"2024-11-21T21:54:18.332376Z","shell.execute_reply.started":"2024-11-21T21:54:18.090261Z","shell.execute_reply":"2024-11-21T21:54:18.331563Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['DQN_01_256_4001027']"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Evaluation","metadata":{"id":"8sYE30Ry5bsn"}},{"cell_type":"code","source":"def eval(new_agent):\n\n  episode_rewards = []\n\n\n\n  action_series = []\n\n  for episode in range(test_episodes):\n\n      action_series.append([])\n\n      state, _ = env_eval.reset()\n\n      episode_reward = 0\n\n      done = False\n\n      while not done:\n\n          action = new_agent.act(state, eps=0.)\n\n          next_state, reward, term, trunc, _ = env_eval.step(action)\n          done = term or trunc\n\n          action_series[-1].append(action)\n\n          episode_reward += reward\n\n          state = next_state\n\n\n\n      episode_rewards.append(episode_reward)\n\n\n\n\n\n  average_reward = sum(episode_rewards) / test_episodes\n\n  return average_reward, action_series","metadata":{"id":"JdLpXclb5dae","execution":{"iopub.status.busy":"2024-11-21T21:55:42.074732Z","iopub.execute_input":"2024-11-21T21:55:42.075197Z","iopub.status.idle":"2024-11-21T21:55:42.084119Z","shell.execute_reply.started":"2024-11-21T21:55:42.075153Z","shell.execute_reply":"2024-11-21T21:55:42.083243Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Training","metadata":{"id":"VEyTwl3aGWTF"}},{"cell_type":"code","source":"for seed in range(1):\n\n  print(f\"seed = {seed}\")\n\n  for i, network in enumerate(networks):\n\n\n\n    if i not in [0]:\n\n      continue\n\n\n\n    buffer = deque(maxlen=buffer_size)\n\n\n\n    new_agent = DQNAgent(input_dim, output_dim, seed=seed, lr = lr, network={\"depth\": depths[i], \"fc_units\": fc_units[i], \"final_fc_units\": final_units[i]})\n\n\n\n    # Training\n\n\n\n    losses = []\n\n    rewards = []\n\n    average_rewards = []\n\n\n\n    ep_mean_reward = 0\n\n    ep_mean_reward_fixed = 0\n\n    ep_mean_loss = 0\n\n    epsilon = epsilon_start\n\n\n\n    pbar = trange(num_episodes, position=0, leave=True, desc=f\"{netwrok_names[i]}\", unit=\"episode\")\n\n\n\n    for episode in pbar:\n\n        # Reset the environment\n\n        state, _ = env.reset()\n\n        epsilon = max(epsilon_end, epsilon * epsilon_decay_rate)\n\n        ep_reward = 0\n\n        ep_loss = 0\n\n        #print(state)\n\n        # Run one episode\n\n        for step in range(max_steps_per_episode_train):\n\n            # Choose and perform an action\n\n            action = new_agent.act(state, epsilon)\n\n            next_state, reward, term, trunc, _ = env.step(action)\n            done = term or trunc\n\n            #if reward > 0:\n\n            #  print(\"REWARDDDDDD!\")\n\n            buffer.append((state, action, reward, next_state, done))\n\n\n\n            #print(len(buffer), batch_size)\n\n            if len(buffer) >= batch_size:\n\n                batch = random.sample(buffer, batch_size)\n\n                # Update the agent's knowledge\n\n                loss = new_agent.learn(batch, gamma)\n\n                ep_loss += loss\n\n\n\n            state = next_state\n\n\n\n            ep_reward += reward\n\n            # Check if the episode has ended\n\n            if done:\n\n              #print(\"H\")\n\n              break\n\n\n\n        rewards.append(ep_reward)\n\n        ep_mean_reward += ep_reward\n\n        if episode % update_frequency == 0:\n\n          ep_mean_reward_fixed = ep_mean_reward\n\n          average_reward, action_series = eval(new_agent)\n\n          ep_mean_reward = 0\n\n\n\n        #if episode % (update_frequency * update_frequency) == 0:\n\n        #  eval_Q_plot(new_agent)\n\n          #print(action_series)\n\n\n\n        average_rewards.append(average_reward)\n\n\n\n        if type(ep_loss) == int:\n\n          record = 0\n\n          pbar.set_postfix(loss=ep_loss, ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n\n          losses.append(ep_loss)\n\n        else:\n\n          record = 1\n\n          pbar.set_postfix(loss=ep_loss.item(), ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n\n          losses.append(ep_loss.detach().cpu())\n\n\n\n    data[network][\"eval_reward\"].append(average_rewards)\n\n    if record == 1:\n\n      data[network][\"loss\"].append(losses)\n\n    data[network][\"train_reward\"].append(rewards)\n\n\n\n    to_be_saved = {\"loss\": np.array(losses).tolist(), \"train_reward\": rewards, \"eval_reward\": average_rewards}\n\n    with open(f\"/kaggle/working/Acrobat_{netwrok_names[i][:7]}{seed}.json\", \"w\") as f:\n\n      json.dump(to_be_saved, f)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"6rpzjfY2IG5-","outputId":"1e680a31-543a-4903-9345-11fc0a637ee9","execution":{"iopub.status.busy":"2024-11-21T21:55:44.234349Z","iopub.execute_input":"2024-11-21T21:55:44.234696Z","iopub.status.idle":"2024-11-21T21:56:29.510717Z","shell.execute_reply.started":"2024-11-21T21:55:44.234666Z","shell.execute_reply":"2024-11-21T21:56:29.509566Z"},"trusted":true},"outputs":[{"name":"stdout","text":"seed = 0\n","output_type":"stream"},{"name":"stderr","text":"DQN_01_256_4001027:   0%|          | 4/1000 [00:45<3:07:03, 11.27s/episode, ep_reward=-500, epsilon=0.961, loss=0.177, train_reward=-150]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 69\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#print(state)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Run one episode\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps_per_episode_train):\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Choose and perform an action\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnew_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     next_state, reward, term, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     72\u001b[0m     done \u001b[38;5;241m=\u001b[39m term \u001b[38;5;129;01mor\u001b[39;00m trunc\n","Cell \u001b[0;32mIn[8], line 55\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m):\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m#print(state)\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":21},{"cell_type":"markdown","source":"# Visualization","metadata":{"id":"ptjV-i4LGaj4"}}]}