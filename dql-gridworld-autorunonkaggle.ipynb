{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["W3Gq3o_bGy10","ArzuUc8WFVOs","joqj9DFyFkcq","6LaO9SJLFo7p","xH3J2_azF2Xh","Wq2snh0iFuqi","fek2zIOuF5Me","8sYE30Ry5bsn"],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Restart the session after running this cell (Run it once!)\n!git clone https://github.com/sparisi/gym_gridworlds\n!pip install ./gym_gridworlds","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlY9Ig2pE5qj","outputId":"c950548c-4d7d-46b5-bec2-b5d785b85ee3","execution":{"iopub.status.busy":"2024-11-14T02:25:56.867025Z","iopub.execute_input":"2024-11-14T02:25:56.867424Z","iopub.status.idle":"2024-11-14T02:26:16.039713Z","shell.execute_reply.started":"2024-11-14T02:25:56.867384Z","shell.execute_reply":"2024-11-14T02:26:16.038669Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'gym_gridworlds'...\nremote: Enumerating objects: 166, done.\u001b[K\nremote: Counting objects: 100% (27/27), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 166 (delta 22), reused 17 (delta 17), pack-reused 139 (from 1)\u001b[K\nReceiving objects: 100% (166/166), 68.22 KiB | 1.34 MiB/s, done.\nResolving deltas: 100% (86/86), done.\nProcessing ./gym_gridworlds\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (from Gym-Gridworlds==1.0) (0.29.0)\nCollecting pygame (from Gym-Gridworlds==1.0)\n  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (3.0.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium->Gym-Gridworlds==1.0) (0.0.4)\nDownloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: Gym-Gridworlds\n  Building wheel for Gym-Gridworlds (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for Gym-Gridworlds: filename=Gym_Gridworlds-1.0-py3-none-any.whl size=15432 sha256=76d786a72ee38fc03f227a9e8a5e900f636e6219a1f1d907493ac42daddd36c1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-w5_ym1x8/wheels/1b/52/46/c69490b45c86839705223517c2823ee0f3c6ed283b7b6a5e93\nSuccessfully built Gym-Gridworlds\nInstalling collected packages: pygame, Gym-Gridworlds\nSuccessfully installed Gym-Gridworlds-1.0 pygame-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import importlib.machinery\nimport imp\nloader = importlib.machinery.SourceFileLoader(\"gym_gridworlds\", '/kaggle/working/gym_gridworlds/gym_gridworlds/__init__.py')\nhandle = loader.load_module(\"gym_gridworlds\")\nimp.reload(handle)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T02:26:16.041909Z","iopub.execute_input":"2024-11-14T02:26:16.042253Z","iopub.status.idle":"2024-11-14T02:26:16.653710Z","shell.execute_reply.started":"2024-11-14T02:26:16.042216Z","shell.execute_reply":"2024-11-14T02:26:16.652693Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2230302866.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n  import imp\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Straight-20-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Empty-2x2-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Empty-3x3-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Empty-Loop-3x3-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Empty-10x10-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Empty-Distract-6x6-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Penalty-3x3-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Quicksand-4x4-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Quicksand-Distract-4x4-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/TwoRoom-Quicksand-3x5-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Corridor-3x4-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Full-4x5-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/TwoRoom-Distract-Middle-2x11-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/Barrier-5x5-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/RiverSwim-6-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/CliffWalk-4x12-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/DangerMaze-6x6-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Gym-Gridworlds/DirtCleaning-10x10-v0 already in registry.\u001b[0m\n  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<module 'gym_gridworlds' from '/opt/conda/lib/python3.10/site-packages/gym_gridworlds/__init__.py'>"},"metadata":{}}]},{"cell_type":"code","source":"import importlib.machinery\nimport imp\nloader = importlib.machinery.SourceFileLoader(\"gym_gridworlds\", '/kaggle/working/gym_gridworlds/gym_gridworlds/gridworld.py')\nhandle = loader.load_module(\"gym_gridworlds\")\nimp.reload(handle)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T02:26:16.655026Z","iopub.execute_input":"2024-11-14T02:26:16.655367Z","iopub.status.idle":"2024-11-14T02:26:16.670679Z","shell.execute_reply.started":"2024-11-14T02:26:16.655329Z","shell.execute_reply":"2024-11-14T02:26:16.669706Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<module 'gym_gridworlds' from '/opt/conda/lib/python3.10/site-packages/gym_gridworlds/__init__.py'>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Register Envoronments in Gym","metadata":{"id":"W3Gq3o_bGy10"}},{"cell_type":"code","source":"from gymnasium.envs.registration import register\n\nregister(\n    id=\"Gym-Gridworlds/Straight-20-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=200,\n    kwargs={\n        \"grid\": \"20_straight\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Empty-2x2-v0\",\n    entry_point=\"gym_gridworlds.gridworld:GridworldRandomStart\",\n    max_episode_steps=10,\n    kwargs={\n        \"grid\": \"2x2_empty\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Empty-3x3-v0\",\n    entry_point=\"gym_gridworlds.gridworld:GridworldRandomStart\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"3x3_empty\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Empty-Loop-3x3-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"3x3_empty_loop\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Empty-10x10-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=100,\n    kwargs={\n        \"grid\": \"10x10_empty\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Empty-Distract-6x6-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"6x6_distract\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Penalty-3x3-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"3x3_penalty\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Quicksand-4x4-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"4x4_quicksand\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Quicksand-Distract-4x4-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"4x4_quicksand_distract\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/TwoRoom-Quicksand-3x5-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"3x5_two_room_quicksand\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Corridor-3x4-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"3x4_corridor\",\n    },\n)\nregister(\n    id=\"Gym-Gridworlds/Full-4x5-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"4x5_full\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/TwoRoom-Distract-Middle-2x11-v0\",\n    entry_point=\"gym_gridworlds.gridworld:GridworldMiddleStart\",\n    max_episode_steps=200,\n    kwargs={\n        \"grid\": \"2x11_two_room_distract\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/Barrier-5x5-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=50,\n    kwargs={\n        \"grid\": \"5x5_barrier\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/RiverSwim-6-v0\",\n    entry_point=\"gym_gridworlds.gridworld:RiverSwim\",\n    max_episode_steps=200,\n    kwargs={\n        \"grid\": \"river_swim_6\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/CliffWalk-4x12-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=200,\n    kwargs={\n        \"grid\": \"4x12_cliffwalk\",\n    },\n)\n\nregister(\n    id=\"Gym-Gridworlds/DangerMaze-6x6-v0\",\n    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n    max_episode_steps=200,\n    kwargs={\n        \"grid\": \"6x6_danger_maze\",\n    },\n)","metadata":{"id":"JngHQiknE_hC","execution":{"iopub.status.busy":"2024-11-14T02:26:41.158677Z","iopub.execute_input":"2024-11-14T02:26:41.159074Z","iopub.status.idle":"2024-11-14T02:26:41.176415Z","shell.execute_reply.started":"2024-11-14T02:26:41.159037Z","shell.execute_reply":"2024-11-14T02:26:41.175559Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"id":"ArzuUc8WFVOs"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\ntorch._dynamo.config.suppress_errors = True\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nfrom collections import namedtuple, deque\nimport gym\nimport gymnasium\nfrom tqdm import trange\nimport matplotlib.pyplot as plt\nimport json","metadata":{"id":"HaIz8uDLTyZY","execution":{"iopub.status.busy":"2024-11-14T02:26:44.644370Z","iopub.execute_input":"2024-11-14T02:26:44.644770Z","iopub.status.idle":"2024-11-14T02:26:49.494187Z","shell.execute_reply.started":"2024-11-14T02:26:44.644730Z","shell.execute_reply":"2024-11-14T02:26:49.493371Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WS8cWkmWGi4K","outputId":"6a69f701-e72e-4549-df57-de058c054b09","execution":{"iopub.status.busy":"2024-11-14T02:26:49.495805Z","iopub.execute_input":"2024-11-14T02:26:49.496205Z","iopub.status.idle":"2024-11-14T02:26:49.554475Z","shell.execute_reply.started":"2024-11-14T02:26:49.496169Z","shell.execute_reply":"2024-11-14T02:26:49.553393Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"Kbdag6O4WFIj","execution":{"iopub.status.busy":"2024-11-14T02:26:49.555699Z","iopub.execute_input":"2024-11-14T02:26:49.556006Z","iopub.status.idle":"2024-11-14T02:26:49.565751Z","shell.execute_reply.started":"2024-11-14T02:26:49.555973Z","shell.execute_reply":"2024-11-14T02:26:49.564900Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Feature Functions","metadata":{"id":"joqj9DFyFkcq"}},{"cell_type":"code","source":"def rbf_features(\n    state,\n    centers,\n    sigmas: float,\n):\n    state = torch.tensor(state, device=device)\n    centers = torch.tensor(centers, device=device)\n    D = centers.shape[0]\n    N = state.shape[0]\n    new_state = state[:, None, :].repeat(1, D, 1)\n    new_center = centers.repeat(N, 1, 1)\n    return torch.exp(-torch.linalg.norm(new_state - new_center, 2, axis=2)**2 / sigmas**2 / 2)\n\n\ndef tile_features(\n    state: np.array,  # (N, S)\n    centers: np.array,  # (D, S)\n    widths: float,\n    offsets: list = [0],  # list of tuples of length S\n) -> np.array:  # (N, D)\n\n    D = np.shape(centers)[0]\n    N = np.shape(state)[0]\n    new_state = np.repeat(state[:, None, :], D, axis=1)\n    output = np.zeros((N, D))\n    for offset in offsets:\n        shifted_center = centers + offset\n        new_center = np.repeat(shifted_center[None, :, :], N, axis=0)\n        output += np.array(np.linalg.norm(new_state - new_center, np.inf, axis=2) < widths, dtype=np.float32)\n\n    return output / len(offsets)\n\n\ndef coarse_features(\n    state: np.array,  # (N, S)\n    centers: np.array,  # (D, S)\n    widths: float,\n    offsets: list = [0],  # list of tuples of length S\n) -> np.array:  # (N, D)\n\n    D = np.shape(centers)[0]\n    N = np.shape(state)[0]\n    new_state = np.repeat(state[:, None, :], D, axis=1)\n    output = np.zeros((N, D))\n    for offset in offsets:\n        shifted_center = centers + offset\n        new_center = np.repeat(shifted_center[None, :, :], N, axis=0)\n        output += np.array(np.linalg.norm(new_state - new_center, 2, axis=2) < widths, dtype=np.float32)\n\n    return output / len(offsets)\n\ndef aggregation_features(state, centers):\n    state = torch.tensor(state, device=device)\n    centers = torch.tensor(centers, device=device)\n\n    distance = torch.sum((state[:, None, :] - centers[None, :, :])**2, dim=-1)\n    return (distance == distance.min(-1, keepdims=True).values) * 1.0  # make it float\n","metadata":{"id":"MHAAP-0zM7YS","execution":{"iopub.status.busy":"2024-11-14T02:26:50.062147Z","iopub.execute_input":"2024-11-14T02:26:50.062855Z","iopub.status.idle":"2024-11-14T02:26:50.077549Z","shell.execute_reply.started":"2024-11-14T02:26:50.062814Z","shell.execute_reply":"2024-11-14T02:26:50.076572Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"s = np.array([[1, 2]])\nc = np.array([[0, 0], [1, 1], [4, 4], [6, 6]])\naggregation_features(s, c)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGcb4NywalLn","outputId":"175c67ae-2281-4bc0-e397-44db55454076","execution":{"iopub.status.busy":"2024-11-14T02:26:52.464502Z","iopub.execute_input":"2024-11-14T02:26:52.465481Z","iopub.status.idle":"2024-11-14T02:26:53.198410Z","shell.execute_reply.started":"2024-11-14T02:26:52.465433Z","shell.execute_reply":"2024-11-14T02:26:53.197433Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[0., 1., 0., 0.]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Network","metadata":{"id":"6LaO9SJLFo7p"}},{"cell_type":"code","source":"# Define the neural network model\nclass QNetwork(nn.Module):\n    def __init__(self, state_size, action_size, seed, depth, fc_unit, final_layer_unit):\n        super().__init__()\n        self.seed = torch.manual_seed(seed)\n        self.feature_size = self.feature_extract_init(state_size, action_size)\n\n        self.layers = nn.ModuleList()\n        for i in range(depth):\n            if depth == 1:\n                self.layers.append(nn.Linear(self.feature_size, final_layer_unit))\n            elif i == 0:\n                self.layers.append(nn.Linear(self.feature_size, fc_unit))\n            elif i == depth - 1:\n                self.layers.append(nn.Linear(fc_unit, final_layer_unit))\n            else:\n                self.layers.append(nn.Linear(fc_unit, fc_unit))\n\n        for layer in self.layers:\n          self.init_weights(layer)\n\n        if depth > 0:\n          self.final_layer = nn.Linear(final_layer_unit, action_size)\n        else:\n          self.final_layer = nn.Linear(self.feature_size, action_size)\n\n        #nn.init.zeros_(self.final_layer.weight)\n        #nn.init.zeros_(self.final_layer.bias)\n        self.init_weights(self.final_layer)\n\n    def init_weights(self, layer):\n        nn.init.xavier_normal(layer.weight)\n        self.to(device)\n\n    def feature_extract_init(self, state_size, action_size):\n        n_centers = [10, 10]\n        centers = np.array(\n          np.meshgrid(*[\n              np.linspace(env.observation_space.low[i], env.observation_space.high[i], n_centers[i])\n              for i in range(env.observation_space.shape[0])\n          ])\n        ).reshape(env.observation_space.shape[0], -1).T\n        centers = torch.tensor(centers).float().to(device)\n        self.feature_name, self.feature_extract = \"Aggregate\", lambda state : aggregation_features(state.reshape(-1, state_size), centers)\n        #self.feature_name, self.feature_extract = \"RBF\", lambda state : rbf_features(state.reshape(-1, state_size), centers, 0.2)\n        return self.feature_extract(env.reset()[0]).shape[1]\n\n\n    def forward(self, state):\n        x = self.feature_extract(state)\n        for layer in self.layers:\n            x = layer(x)\n        return self.final_layer(x)\n","metadata":{"id":"k6kdTlQtHlmF","execution":{"iopub.status.busy":"2024-11-14T02:26:53.814498Z","iopub.execute_input":"2024-11-14T02:26:53.814899Z","iopub.status.idle":"2024-11-14T02:26:53.829454Z","shell.execute_reply.started":"2024-11-14T02:26:53.814863Z","shell.execute_reply":"2024-11-14T02:26:53.828540Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# ReplayBuffer","metadata":{"id":"xH3J2_azF2Xh"}},{"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)\n        self.batch_size = batch_size\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        self.seed = random.seed(seed)\n\n    def add(self, state, action, reward, next_state, done):\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n\n    def sample(self):\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"id":"WUEjnATTF0PM","execution":{"iopub.status.busy":"2024-11-14T02:26:55.626985Z","iopub.execute_input":"2024-11-14T02:26:55.627691Z","iopub.status.idle":"2024-11-14T02:26:55.638667Z","shell.execute_reply.started":"2024-11-14T02:26:55.627647Z","shell.execute_reply":"2024-11-14T02:26:55.637665Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# DQN","metadata":{"id":"Wq2snh0iFuqi"}},{"cell_type":"code","source":"# Define the DQN agent class\nclass DQNAgent:\n    # Initialize the DQN agent\n    def __init__(self, state_size, action_size, seed, lr, network):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(seed)\n\n        self.qnetwork_local = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n        self.qnetwork_target = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n\n        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n        self.t_step = 0\n\n    def step(self, state, action, reward, next_state, done):\n        self.memory.add(state, action, reward, next_state, done)\n\n        self.t_step = (self.t_step + 1) % 4\n        if self.t_step == 0:\n            if len(self.memory) > 64:\n                experiences = self.memory.sample()\n                self.learn(experiences, gamma=0.99)\n\n    # Choose an action based on the current state\n    def act(self, state, eps=0.):\n        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state_tensor)\n        self.qnetwork_local.train()\n\n        if np.random.random() > eps:\n            return action_values.argmax(dim=1).item()\n        else:\n            return np.random.randint(self.action_size)\n\n    # Learn from batch of experiences\n    def learn(self, experiences, gamma):\n        states, actions, rewards, next_states, dones = zip(*experiences)\n        states = torch.from_numpy(np.vstack(states)).float().to(device)\n        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n\n        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n\n        Q_expected = self.qnetwork_local(states).gather(1, actions)\n\n        loss = F.mse_loss(Q_expected, Q_targets)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n\n        return loss\n\n    def soft_update(self, local_model, target_model, tau):\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)","metadata":{"id":"SMqPt_ruIG59","execution":{"iopub.status.busy":"2024-11-14T02:26:56.955665Z","iopub.execute_input":"2024-11-14T02:26:56.956053Z","iopub.status.idle":"2024-11-14T02:26:56.973804Z","shell.execute_reply.started":"2024-11-14T02:26:56.956015Z","shell.execute_reply":"2024-11-14T02:26:56.972785Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{"id":"WxOD0rGTF_Ee"}},{"cell_type":"code","source":"num_episodes = 4000\nmax_steps_per_episode_train = 100\nmax_steps_per_episode_eval = 25\nepsilon_start = 1.0\nepsilon_end = 0.05\nepsilon_decay_rate = 0.995\ngamma = 0.9\nlr = 1e-4\nbuffer_size = 10000\nbatch_size = 512\nupdate_frequency = 10\ntest_episodes = 1","metadata":{"id":"ztLArg8OF-4o","execution":{"iopub.status.busy":"2024-11-14T02:26:58.166094Z","iopub.execute_input":"2024-11-14T02:26:58.166964Z","iopub.status.idle":"2024-11-14T02:26:58.172407Z","shell.execute_reply.started":"2024-11-14T02:26:58.166915Z","shell.execute_reply":"2024-11-14T02:26:58.171414Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Setup Environment","metadata":{"id":"fek2zIOuF5Me"}},{"cell_type":"code","source":"env_id = \"Gym-Gridworlds/DangerMaze-6x6-v0\"\n#env_id = \"Gym-Gridworlds/Empty-10x10-v0\"\n#env_id = \"Gym-Gridworlds/Penalty-3x3-v0\"\nenv = gymnasium.make(env_id, coordinate_observation=True)\nenv_eval = gymnasium.make(env_id, coordinate_observation=True, max_episode_steps=max_steps_per_episode_eval)","metadata":{"id":"nSD8eiX5IG5-","execution":{"iopub.status.busy":"2024-11-14T02:26:59.764406Z","iopub.execute_input":"2024-11-14T02:26:59.765313Z","iopub.status.idle":"2024-11-14T02:26:59.775807Z","shell.execute_reply.started":"2024-11-14T02:26:59.765259Z","shell.execute_reply":"2024-11-14T02:26:59.774276Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# DQN Agent","metadata":{"id":"R_nWh5QTGSUN"}},{"cell_type":"code","source":"input_dim = env.observation_space.shape[0]\noutput_dim = env.action_space.n\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50]\nfc_units = [4096, 608, 440, 363, 316, 284, 260, 241, 226, 213, 148, 93]\nfinal_units = [4102, 608, 440, 363, 318, 284, 260, 245, 228, 222, 149, 58]\nnetworks = [QNetwork(input_dim, output_dim, seed=i, depth=depths[i], fc_unit=fc_units[i], final_layer_unit=final_units[i]) for i in range(len(depths))]\nnetwrok_names = [f\"DQN_{depths[i]:02}_{fc_units[i]}_{sum(p.numel() for p in networks[i].parameters())}\" for i in range(len(depths))]\ndata = {}\nfor network in networks:\n  data[network] = {\"loss\": [], \"train_reward\": [], \"eval_reward\": []}\n\nnetwrok_names","metadata":{"id":"H2pzwB0CRL7X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c18a13a-4a20-4e5a-d5af-4d46092940d6","execution":{"iopub.status.busy":"2024-11-14T01:54:56.824724Z","iopub.execute_input":"2024-11-14T01:54:56.825141Z","iopub.status.idle":"2024-11-14T01:54:57.030790Z","shell.execute_reply.started":"2024-11-14T01:54:56.825103Z","shell.execute_reply":"2024-11-14T01:54:57.029829Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['DQN_01_4096_434817',\n 'DQN_02_608_434725',\n 'DQN_03_440_434725',\n 'DQN_04_363_434879',\n 'DQN_05_316_434833',\n 'DQN_06_284_434809',\n 'DQN_07_260_434725',\n 'DQN_08_241_434793',\n 'DQN_09_226_434841',\n 'DQN_10_213_434792',\n 'DQN_20_148_434835',\n 'DQN_50_93_434756']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{"id":"8sYE30Ry5bsn"}},{"cell_type":"code","source":"def eval(new_agent):\n  episode_rewards = []\n\n  action_series = []\n  for episode in range(test_episodes):\n      state, _ = env_eval.reset()\n      episode_reward = 0\n      done = False\n      while not done:\n          action = new_agent.act(state, eps=0.)\n          next_state, reward, terminated, truncated, _ = env_eval.step(action)\n          action_series.append(action)\n          done = terminated or truncated\n          episode_reward += reward\n          state = next_state\n\n      episode_rewards.append(episode_reward)\n\n\n  average_reward = sum(episode_rewards) / test_episodes\n  return average_reward, action_series","metadata":{"id":"JdLpXclb5dae","execution":{"iopub.status.busy":"2024-11-14T01:55:03.143350Z","iopub.execute_input":"2024-11-14T01:55:03.144198Z","iopub.status.idle":"2024-11-14T01:55:03.151257Z","shell.execute_reply.started":"2024-11-14T01:55:03.144157Z","shell.execute_reply":"2024-11-14T01:55:03.149976Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def eval_Q_plot(new_agent):\n  s = 6\n  b = torch.tensor(np.array([[i, j] for j in range(s-1) for i in range(s)])).to(device)\n  res = new_agent.qnetwork_local(b)\n  a = np.zeros((5, s-1, s))\n  for i in range(s-1):\n    for j in range(s):\n      a[0, i, j] = res[i * (s-1) + j][0]\n      a[1, i, j] = res[i * (s-1) + j][1]\n      a[2, i, j] = res[i * (s-1) + j][2]\n      a[3, i, j] = res[i * (s-1) + j][3]\n      a[4, i, j] = res[i * (s-1) + j][4]\n\n  fig, ax = plt.subplots(ncols=5)\n\n  ax[0].imshow(a[0], cmap='hot', interpolation='nearest')\n  ax[1].imshow(a[1], cmap='hot', interpolation='nearest')\n  ax[2].imshow(a[2], cmap='hot', interpolation='nearest')\n  ax[3].imshow(a[3], cmap='hot', interpolation='nearest')\n  ax[4].imshow(a[4], cmap='hot', interpolation='nearest')\n\n  ax[0].set_title(\"Action 0\")\n  ax[1].set_title(\"Action 1\")\n  ax[2].set_title(\"Action 2\")\n  ax[3].set_title(\"Action 3\")\n  ax[4].set_title(\"Action 4\")\n\n  plt.tight_layout()\n  plt.show()","metadata":{"id":"OpZqj7q36dge","execution":{"iopub.status.busy":"2024-11-14T01:55:05.594204Z","iopub.execute_input":"2024-11-14T01:55:05.594641Z","iopub.status.idle":"2024-11-14T01:55:05.606819Z","shell.execute_reply.started":"2024-11-14T01:55:05.594597Z","shell.execute_reply":"2024-11-14T01:55:05.605608Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"VEyTwl3aGWTF"}},{"cell_type":"code","source":"for seed in range(5):\n  print(f\"seed = {seed}\")\n  for i, network in enumerate(networks):\n\n    if i not in [2, 10, 11]:\n      continue\n\n    buffer = deque(maxlen=buffer_size)\n\n    new_agent = DQNAgent(input_dim, output_dim, seed=seed, lr = lr, network={\"depth\": depths[i], \"fc_units\": fc_units[i], \"final_fc_units\": final_units[i]})\n\n    # Training\n\n    losses = []\n    rewards = []\n    average_rewards = []\n\n    ep_mean_reward = 0\n    ep_mean_reward_fixed = 0\n    ep_mean_loss = 0\n    epsilon = epsilon_start\n\n    pbar = trange(num_episodes, position=0, leave=True, desc=f\"{netwrok_names[i]}\", unit=\"episode\")\n\n    for episode in pbar:\n        # Reset the environment\n        state, _ = env.reset()\n        epsilon = max(epsilon_end, epsilon * epsilon_decay_rate)\n\n        ep_reward = 0\n        ep_loss = 0\n        # Run one episode\n        for step in range(max_steps_per_episode_train):\n            # Choose and perform an action\n            action = new_agent.act(state, epsilon)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            #if reward > 0:\n            #  print(\"REWARDDDDDD!\")\n            buffer.append((state, action, reward, next_state, done))\n\n            #print(len(buffer), batch_size)\n            if len(buffer) >= batch_size:\n                batch = random.sample(buffer, batch_size)\n                # Update the agent's knowledge\n                loss = new_agent.learn(batch, gamma)\n                ep_loss += loss\n\n            state = next_state\n\n            ep_reward += reward\n            # Check if the episode has ended\n            if done:\n                break\n\n        rewards.append(ep_reward)\n        ep_mean_reward += ep_reward\n        if episode % update_frequency == 0:\n          ep_mean_reward_fixed = ep_mean_reward\n          average_reward, action_series = eval(new_agent)\n          ep_mean_reward = 0\n\n        #if episode % (update_frequency * update_frequency) == 0:\n          #eval_Q_plot(new_agent)\n          #print(action_series)\n\n        average_rewards.append(average_reward)\n\n        if type(ep_loss) == int:\n          record = 0\n          pbar.set_postfix(loss=ep_loss, ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n          losses.append(ep_loss)\n        else:\n          record = 1\n          pbar.set_postfix(loss=ep_loss.item(), ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n          losses.append(ep_loss.detach().cpu())\n\n    data[network][\"eval_reward\"].append(average_rewards)\n    if record == 1:\n      data[network][\"loss\"].append(losses)\n    data[network][\"train_reward\"].append(rewards)\n\n    to_be_saved = {\"loss\": np.array(losses).tolist(), \"train_reward\": rewards, \"eval_reward\": average_rewards}\n    with open(f\"/kaggle/working/{env_id[15:25]}_{netwrok_names[i][:7]}{seed}.json\", \"w\") as f:\n      json.dump(to_be_saved, f)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"6rpzjfY2IG5-","outputId":"f993bcc6-2e60-4a7f-f120-7261af57dcde","execution":{"iopub.status.busy":"2024-11-14T01:56:16.502980Z","iopub.execute_input":"2024-11-14T01:56:16.503402Z","iopub.status.idle":"2024-11-14T02:00:15.912252Z","shell.execute_reply.started":"2024-11-14T01:56:16.503364Z","shell.execute_reply":"2024-11-14T02:00:15.910819Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"seed = 0\n","output_type":"stream"},{"name":"stderr","text":"DQN_03_440_434725:  18%|█▊        | 726/4000 [03:58<17:55,  3.04episode/s, ep_reward=0, epsilon=0.05, loss=0.0203, train_reward=-9]      \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(buffer, batch_size)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Update the agent's knowledge\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mnew_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     ep_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     49\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n","Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m     47\u001b[0m Q_targets_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_target(next_states)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m Q_targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m Q_targets_next \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones))\n\u001b[0;32m---> 50\u001b[0m Q_expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnetwork_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(Q_expected, Q_targets)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 53\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Visualization","metadata":{"id":"ptjV-i4LGaj4"}}]}