{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W3Gq3o_bGy10",
        "ArzuUc8WFVOs",
        "joqj9DFyFkcq",
        "6LaO9SJLFo7p",
        "xH3J2_azF2Xh",
        "Wq2snh0iFuqi",
        "fek2zIOuF5Me",
        "8sYE30Ry5bsn"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Restart the session after running this cell (Run it once!)\n",
        "!git clone https://github.com/sparisi/gym_gridworlds\n",
        "!pip install ./gym_gridworlds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlY9Ig2pE5qj",
        "outputId": "f96c1f99-a966-48c2-e347-e9728ffe8d98",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:25:56.867025Z",
          "iopub.execute_input": "2024-11-14T02:25:56.867424Z",
          "iopub.status.idle": "2024-11-14T02:26:16.039713Z",
          "shell.execute_reply.started": "2024-11-14T02:25:56.867384Z",
          "shell.execute_reply": "2024-11-14T02:26:16.038669Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gym_gridworlds'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 166 (delta 22), reused 17 (delta 17), pack-reused 139 (from 1)\u001b[K\n",
            "Receiving objects: 100% (166/166), 68.22 KiB | 478.00 KiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "Processing ./gym_gridworlds\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gymnasium (from Gym-Gridworlds==1.0)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from Gym-Gridworlds==1.0) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium->Gym-Gridworlds==1.0)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: Gym-Gridworlds\n",
            "  Building wheel for Gym-Gridworlds (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Gym-Gridworlds: filename=Gym_Gridworlds-1.0-py3-none-any.whl size=15432 sha256=80438981fb14ba6fffb491c5ec8cb569ed130cd0e6a986fa005755688abc99a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r6orqwyp/wheels/35/b6/53/d1fcfbb39d514897bf6fbae43c1b74a9f3c71ff4f92c6d32d3\n",
            "Successfully built Gym-Gridworlds\n",
            "Installing collected packages: farama-notifications, gymnasium, Gym-Gridworlds\n",
            "Successfully installed Gym-Gridworlds-1.0 farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ArzuUc8WFVOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import gym\n",
        "import gymnasium\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ],
      "metadata": {
        "id": "HaIz8uDLTyZY",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:44.644370Z",
          "iopub.execute_input": "2024-11-14T02:26:44.644770Z",
          "iopub.status.idle": "2024-11-14T02:26:49.494187Z",
          "shell.execute_reply.started": "2024-11-14T02:26:44.644730Z",
          "shell.execute_reply": "2024-11-14T02:26:49.493371Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS8cWkmWGi4K",
        "outputId": "70b79463-7d1a-438e-ef53-ec7ed1b089c1",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:49.495805Z",
          "iopub.execute_input": "2024-11-14T02:26:49.496205Z",
          "iopub.status.idle": "2024-11-14T02:26:49.554475Z",
          "shell.execute_reply.started": "2024-11-14T02:26:49.496169Z",
          "shell.execute_reply": "2024-11-14T02:26:49.553393Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Kbdag6O4WFIj",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:49.555699Z",
          "iopub.execute_input": "2024-11-14T02:26:49.556006Z",
          "iopub.status.idle": "2024-11-14T02:26:49.565751Z",
          "shell.execute_reply.started": "2024-11-14T02:26:49.555973Z",
          "shell.execute_reply": "2024-11-14T02:26:49.564900Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "6LaO9SJLFo7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tile_features(\n",
        "    state,\n",
        "    centers,\n",
        "    widths,\n",
        "    offsets: list = [0]\n",
        "):\n",
        "    state = torch.tensor(state).float().to(device)\n",
        "    centers = torch.tensor(centers).float().to(device)\n",
        "    widths = torch.tensor(widths).float().to(device)\n",
        "\n",
        "    D = centers.shape[0]\n",
        "    N = state.shape[0]\n",
        "\n",
        "    #new_state = np.repeat(state[:, None, :], D, axis=1)\n",
        "    new_state = state[:, None, :].repeat(1, D, 1)\n",
        "\n",
        "    #widths = widths[None, None, :].repeat(N, D, 1)\n",
        "    #output = np.zeros((N, D))\n",
        "    output = torch.zeros((N, D), device=state.device)\n",
        "    for offset in offsets:\n",
        "        shifted_center = centers + offset\n",
        "        new_center = shifted_center[None, :, :].repeat(N, 1, 1)\n",
        "        #new_center = np.repeat(shifted_center[None, :, :], N, axis=0)\n",
        "        #print(new_center.shape, torch.norm(new_state - new_center, float('inf'), dim=2).shape, widths.shape)\n",
        "        output += (torch.norm(new_state - new_center, float('2'), dim=2) < widths).type(torch.float32)\n",
        "        #output += np.array(np.linalg.norm(new_state - new_center, np.inf, axis=2) < widths, dtype=np.float32)\n",
        "\n",
        "    return output / len(offsets)"
      ],
      "metadata": {
        "id": "zhHTgT2-Hjwc"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, depth, fc_unit, final_layer_unit):\n",
        "        super().__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.feature_size = self.feature_extract_init(state_size, action_size)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            if depth == 1:\n",
        "                self.layers.append(nn.Linear(self.feature_size, final_layer_unit))\n",
        "            elif i == 0:\n",
        "                self.layers.append(nn.Linear(self.feature_size, fc_unit))\n",
        "            elif i == depth - 1:\n",
        "                self.layers.append(nn.Linear(fc_unit, final_layer_unit))\n",
        "            else:\n",
        "                self.layers.append(nn.Linear(fc_unit, fc_unit))\n",
        "\n",
        "        for layer in self.layers:\n",
        "          self.init_weights(layer)\n",
        "\n",
        "        if depth > 0:\n",
        "          self.final_layer = nn.Linear(final_layer_unit, action_size)\n",
        "        else:\n",
        "          self.final_layer = nn.Linear(self.feature_size, action_size)\n",
        "\n",
        "        #nn.init.zeros_(self.final_layer.weight)\n",
        "        #nn.init.zeros_(self.final_layer.bias)\n",
        "        self.init_weights(self.final_layer)\n",
        "\n",
        "    def init_weights(self, layer):\n",
        "        nn.init.xavier_normal(layer.weight)\n",
        "        self.to(device)\n",
        "\n",
        "    def feature_extract_init(self, state_size, action_size):\n",
        "        #low = torch.tensor(np.array([-4.8, -20, -4.8, -20]), device=device, dtype=torch.float32) # Convert to tensor and move to device\n",
        "        #high = torch.tensor(np.array([4.8, 20, 4.8, 20]), device=device, dtype=torch.float32)  # Convert to tensor and move to device\n",
        "        low = torch.tensor(env.observation_space.low, device=device, dtype=torch.float32)\n",
        "        high = torch.tensor(env.observation_space.high, device=device, dtype=torch.float32)\n",
        "        n_bins = 7\n",
        "        n_centers = np.repeat(n_bins, env.observation_space.shape[0])\n",
        "        offset_pos = np.linspace(-1, 1, 3 * n_bins)\n",
        "        #print(offset_pos)\n",
        "        centers = np.array(\n",
        "            np.meshgrid(*[\n",
        "                np.linspace(-1, 1, n_centers[i])\n",
        "                for i in range(env.observation_space.shape[0])\n",
        "            ])\n",
        "        ).reshape(env.observation_space.shape[0], -1).T\n",
        "        centers = torch.tensor(centers, device=device, dtype=torch.float32)  # Convert to tensor and move to device\n",
        "\n",
        "        self.feature_name, self.feature_extract = \"Tile\", lambda state: tile_features(((torch.tensor(state).to(device) - low) / (high - low)).reshape(-1, state_size), centers, 1.0, offset_pos)\n",
        "        return self.feature_extract(env.reset()).shape[1]\n",
        "    # def feature_extract_init(self, state_size, action_size):\n",
        "    #     low = torch.tensor(np.array([-4.8, -10, -4.8, -10]))\n",
        "    #     high = torch.tensor(np.array([4.8, 10, 4.8, 10]))\n",
        "    #     n_bins = 10\n",
        "    #     n_centers = np.repeat(n_bins, env.observation_space.shape[0])\n",
        "    #     offset_pos = 2/(3*n_bins)\n",
        "    #     centers = np.array(\n",
        "    #       np.meshgrid(*[\n",
        "    #           np.linspace(-1, 1, n_centers[i])\n",
        "    #           for i in range(env.observation_space.shape[0])\n",
        "    #       ])\n",
        "    #     ).reshape(env.observation_space.shape[0], -1).T\n",
        "    #     centers = torch.tensor(centers).float().to(device)\n",
        "    #     #widths = 1.2 * (env.observation_space.high - env.observation_space.low)/(n_bins)\n",
        "    #     #print(centers.shape, state.shape)\n",
        "    #     #self.feature_name, self.feature_extract = \"Aggregate\", lambda state : aggregation_features(state.reshape(-1, state_size), centers)\n",
        "    #     #self.feature_name, self.feature_extract = \"RBF\", lambda state : rbf_features(state.reshape(-1, state_size), centers, 0.2)\n",
        "    #     self.feature_name, self.feature_extract = \"Tile\", lambda state : tile_features(((state - low) / (high - low)).reshape(-1, state_size), centers, 0.5)\n",
        "    #     #print(env.reset())\n",
        "    #     #print(self.feature_extract(env.reset()).shape)\n",
        "    #     return self.feature_extract(env.reset()).shape[1]\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        #print(state.shape)\n",
        "        #print(state)\n",
        "        x = self.feature_extract(state)\n",
        "        #print(len(x[0]), torch.sum(x), torch.sum(x > 0))\n",
        "        #x = state\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.final_layer(x)\n"
      ],
      "metadata": {
        "id": "k6kdTlQtHlmF",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:53.814498Z",
          "iopub.execute_input": "2024-11-14T02:26:53.814899Z",
          "iopub.status.idle": "2024-11-14T02:26:53.829454Z",
          "shell.execute_reply.started": "2024-11-14T02:26:53.814863Z",
          "shell.execute_reply": "2024-11-14T02:26:53.828540Z"
        },
        "trusted": true
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReplayBuffer"
      ],
      "metadata": {
        "id": "xH3J2_azF2Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "WUEjnATTF0PM",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:55.626985Z",
          "iopub.execute_input": "2024-11-14T02:26:55.627691Z",
          "iopub.status.idle": "2024-11-14T02:26:55.638667Z",
          "shell.execute_reply.started": "2024-11-14T02:26:55.627647Z",
          "shell.execute_reply": "2024-11-14T02:26:55.637665Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "Wq2snh0iFuqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DQN agent class\n",
        "class DQNAgent:\n",
        "    # Initialize the DQN agent\n",
        "    def __init__(self, state_size, action_size, seed, lr, network):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed, network[\"depth\"], network[\"fc_units\"], network[\"final_fc_units\"]).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > 64:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, gamma=0.99)\n",
        "\n",
        "    # Choose an action based on the current state\n",
        "    def act(self, state, eps=0.):\n",
        "        #print(state)\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state_tensor)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if np.random.random() > eps:\n",
        "            return action_values.argmax(dim=1).item()\n",
        "        else:\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "    # Learn from batch of experiences\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "SMqPt_ruIG59",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:56.955665Z",
          "iopub.execute_input": "2024-11-14T02:26:56.956053Z",
          "iopub.status.idle": "2024-11-14T02:26:56.973804Z",
          "shell.execute_reply.started": "2024-11-14T02:26:56.956015Z",
          "shell.execute_reply": "2024-11-14T02:26:56.972785Z"
        },
        "trusted": true
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "WxOD0rGTF_Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "max_steps_per_episode_train = 150\n",
        "#max_steps_per_episode_eval = 25\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay_rate = 0.99\n",
        "gamma = 0.99\n",
        "lr = 1e-5\n",
        "buffer_size = 10000\n",
        "batch_size = 256\n",
        "update_frequency = 5\n",
        "test_episodes = 1"
      ],
      "metadata": {
        "id": "ztLArg8OF-4o",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:58.166094Z",
          "iopub.execute_input": "2024-11-14T02:26:58.166964Z",
          "iopub.status.idle": "2024-11-14T02:26:58.172407Z",
          "shell.execute_reply.started": "2024-11-14T02:26:58.166915Z",
          "shell.execute_reply": "2024-11-14T02:26:58.171414Z"
        },
        "trusted": true
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Environment"
      ],
      "metadata": {
        "id": "fek2zIOuF5Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#env_id = \"Gym-Gridworlds/DangerMaze-6x6-v0\"\n",
        "#env_id = \"Gym-Gridworlds/Empty-10x10-v0\"\n",
        "#env_id = \"Gym-Gridworlds/Penalty-3x3-v0\"\n",
        "#env = gymnasium.make(env_id, render_mode=\"binary\")\n",
        "#env_eval = gymnasium.make(env_id, max_episode_steps=max_steps_per_episode_eval, render_mode=\"binary\")\n",
        "env = gym.make('Acrobot-v1')\n",
        "env_eval = gym.make('Acrobot-v1')"
      ],
      "metadata": {
        "id": "nSD8eiX5IG5-",
        "execution": {
          "iopub.status.busy": "2024-11-14T02:26:59.764406Z",
          "iopub.execute_input": "2024-11-14T02:26:59.765313Z",
          "iopub.status.idle": "2024-11-14T02:26:59.775807Z",
          "shell.execute_reply.started": "2024-11-14T02:26:59.765259Z",
          "shell.execute_reply": "2024-11-14T02:26:59.774276Z"
        },
        "trusted": true
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Agent"
      ],
      "metadata": {
        "id": "R_nWh5QTGSUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "#depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50]\n",
        "#fc_units = [500, 366, 262, 215, 187, 168, 153, 142, 133, 125, 87, 54]\n",
        "#final_units = [500, 366, 263, 217, 188, 165, 157, 144, 134, 135, 75, 54]\n",
        "depths = [1]\n",
        "fc_units = [64]\n",
        "final_units = [64]\n",
        "networks = [QNetwork(input_dim, output_dim, seed=i, depth=depths[i], fc_unit=fc_units[i], final_layer_unit=final_units[i]) for i in range(len(depths))]\n",
        "netwrok_names = [f\"DQN_{depths[i]:02}_{fc_units[i]}_{sum(p.numel() for p in networks[i].parameters())}\" for i in range(len(depths))]\n",
        "data = {}\n",
        "for network in networks:\n",
        "  data[network] = {\"loss\": [], \"train_reward\": [], \"eval_reward\": []}\n",
        "\n",
        "netwrok_names"
      ],
      "metadata": {
        "id": "H2pzwB0CRL7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cadb47da-dbef-4cd0-abe0-1bc036026b7b",
        "execution": {
          "iopub.status.busy": "2024-11-14T01:54:56.824724Z",
          "iopub.execute_input": "2024-11-14T01:54:56.825141Z",
          "iopub.status.idle": "2024-11-14T01:54:57.030790Z",
          "shell.execute_reply.started": "2024-11-14T01:54:56.825103Z",
          "shell.execute_reply": "2024-11-14T01:54:57.029829Z"
        },
        "trusted": true
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DQN_01_64_7529795']"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "8sYE30Ry5bsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(new_agent):\n",
        "  episode_rewards = []\n",
        "\n",
        "  action_series = []\n",
        "  for episode in range(test_episodes):\n",
        "      action_series.append([])\n",
        "      state = env_eval.reset()\n",
        "      episode_reward = 0\n",
        "      done = False\n",
        "      while not done:\n",
        "          action = new_agent.act(state, eps=0.)\n",
        "          next_state, reward, done, _ = env_eval.step(action)\n",
        "          action_series[-1].append(action)\n",
        "          episode_reward += reward\n",
        "          state = next_state\n",
        "\n",
        "      episode_rewards.append(episode_reward)\n",
        "\n",
        "\n",
        "  average_reward = sum(episode_rewards) / test_episodes\n",
        "  return average_reward, action_series"
      ],
      "metadata": {
        "id": "JdLpXclb5dae",
        "execution": {
          "iopub.status.busy": "2024-11-14T01:55:03.143350Z",
          "iopub.execute_input": "2024-11-14T01:55:03.144198Z",
          "iopub.status.idle": "2024-11-14T01:55:03.151257Z",
          "shell.execute_reply.started": "2024-11-14T01:55:03.144157Z",
          "shell.execute_reply": "2024-11-14T01:55:03.149976Z"
        },
        "trusted": true
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "VEyTwl3aGWTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in range(1):\n",
        "  print(f\"seed = {seed}\")\n",
        "  for i, network in enumerate(networks):\n",
        "\n",
        "    if i not in [0]:\n",
        "      continue\n",
        "\n",
        "    buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    new_agent = DQNAgent(input_dim, output_dim, seed=seed, lr = lr, network={\"depth\": depths[i], \"fc_units\": fc_units[i], \"final_fc_units\": final_units[i]})\n",
        "\n",
        "    # Training\n",
        "\n",
        "    losses = []\n",
        "    rewards = []\n",
        "    average_rewards = []\n",
        "\n",
        "    ep_mean_reward = 0\n",
        "    ep_mean_reward_fixed = 0\n",
        "    ep_mean_loss = 0\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    pbar = trange(num_episodes, position=0, leave=True, desc=f\"{netwrok_names[i]}\", unit=\"episode\")\n",
        "\n",
        "    for episode in pbar:\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay_rate)\n",
        "        ep_reward = 0\n",
        "        ep_loss = 0\n",
        "        #print(state)\n",
        "        # Run one episode\n",
        "        for step in range(max_steps_per_episode_train):\n",
        "            # Choose and perform an action\n",
        "            action = new_agent.act(state, epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            #if reward > 0:\n",
        "            #  print(\"REWARDDDDDD!\")\n",
        "            buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "            #print(len(buffer), batch_size)\n",
        "            if len(buffer) >= batch_size:\n",
        "                batch = random.sample(buffer, batch_size)\n",
        "                # Update the agent's knowledge\n",
        "                loss = new_agent.learn(batch, gamma)\n",
        "                ep_loss += loss\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            ep_reward += reward\n",
        "            # Check if the episode has ended\n",
        "            if done:\n",
        "              #print(\"H\")\n",
        "              break\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        ep_mean_reward += ep_reward\n",
        "        if episode % update_frequency == 0:\n",
        "          ep_mean_reward_fixed = ep_mean_reward\n",
        "          average_reward, action_series = eval(new_agent)\n",
        "          ep_mean_reward = 0\n",
        "\n",
        "        #if episode % (update_frequency * update_frequency) == 0:\n",
        "        #  eval_Q_plot(new_agent)\n",
        "          #print(action_series)\n",
        "\n",
        "        average_rewards.append(average_reward)\n",
        "\n",
        "        if type(ep_loss) == int:\n",
        "          record = 0\n",
        "          pbar.set_postfix(loss=ep_loss, ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n",
        "          losses.append(ep_loss)\n",
        "        else:\n",
        "          record = 1\n",
        "          pbar.set_postfix(loss=ep_loss.item(), ep_reward=average_reward, train_reward=ep_reward, epsilon=epsilon)\n",
        "          losses.append(ep_loss.detach().cpu())\n",
        "\n",
        "    data[network][\"eval_reward\"].append(average_rewards)\n",
        "    if record == 1:\n",
        "      data[network][\"loss\"].append(losses)\n",
        "    data[network][\"train_reward\"].append(rewards)\n",
        "\n",
        "    to_be_saved = {\"loss\": np.array(losses).tolist(), \"train_reward\": rewards, \"eval_reward\": average_rewards}\n",
        "    with open(f\"./Acrobat_{netwrok_names[i][:7]}{seed}.json\", \"w\") as f:\n",
        "      json.dump(to_be_saved, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "6rpzjfY2IG5-",
        "outputId": "1e680a31-543a-4903-9345-11fc0a637ee9",
        "execution": {
          "iopub.status.busy": "2024-11-14T01:56:16.502980Z",
          "iopub.execute_input": "2024-11-14T01:56:16.503402Z",
          "iopub.status.idle": "2024-11-14T02:00:15.912252Z",
          "shell.execute_reply.started": "2024-11-14T01:56:16.503364Z",
          "shell.execute_reply": "2024-11-14T02:00:15.910819Z"
        },
        "trusted": true
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DQN_01_64_7529795:   0%|          | 1/1000 [00:08<2:21:30,  8.50s/episode, ep_reward=-500, epsilon=0.99, loss=0, train_reward=-150]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-196-4c0f7ac7a632>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# Update the agent's knowledge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mep_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-ce5c61012627>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mQ_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_expected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-1c267682d737>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#print(state.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m#print(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m#print(len(x[0]), torch.sum(x), torch.sum(x > 0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#x = state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-1c267682d737>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to tensor and move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtile_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# def feature_extract_init(self, state_size, action_size):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-af093c23b2da>\u001b[0m in \u001b[0;36mtile_features\u001b[0;34m(state, centers, widths, offsets)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mwidths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "ptjV-i4LGaj4"
      }
    }
  ]
}