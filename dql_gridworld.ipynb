{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import namedtuple, deque"
      ],
      "metadata": {
        "id": "HaIz8uDLTyZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rbf_features(\n",
        "    state: np.array,  # (N, S)\n",
        "    centers: np.array,  # (D, S)\n",
        "    sigmas: float,\n",
        ") -> np.array:  # (N, D)\n",
        "\n",
        "    D = np.shape(centers)[0]\n",
        "    N = np.shape(state)[0]\n",
        "    new_state = np.repeat(state[:, None, :], D, axis=1)\n",
        "    new_center = np.repeat(centers[None, :, :], N, axis=0)\n",
        "    return np.exp(-np.linalg.norm(new_state - new_center, 2, axis=2)**2 / sigmas**2 / 2)\n",
        "\n",
        "\n",
        "def tile_features(\n",
        "    state: np.array,  # (N, S)\n",
        "    centers: np.array,  # (D, S)\n",
        "    widths: float,\n",
        "    offsets: list = [0],  # list of tuples of length S\n",
        ") -> np.array:  # (N, D)\n",
        "\n",
        "    D = np.shape(centers)[0]\n",
        "    N = np.shape(state)[0]\n",
        "    new_state = np.repeat(state[:, None, :], D, axis=1)\n",
        "    output = np.zeros((N, D))\n",
        "    for offset in offsets:\n",
        "        shifted_center = centers + offset\n",
        "        new_center = np.repeat(shifted_center[None, :, :], N, axis=0)\n",
        "        output += np.array(np.linalg.norm(new_state - new_center, np.inf, axis=2) < widths, dtype=np.float32)\n",
        "\n",
        "    return output / len(offsets)\n",
        "\n",
        "\n",
        "def coarse_features(\n",
        "    state: np.array,  # (N, S)\n",
        "    centers: np.array,  # (D, S)\n",
        "    widths: float,\n",
        "    offsets: list = [0],  # list of tuples of length S\n",
        ") -> np.array:  # (N, D)\n",
        "\n",
        "    D = np.shape(centers)[0]\n",
        "    N = np.shape(state)[0]\n",
        "    new_state = np.repeat(state[:, None, :], D, axis=1)\n",
        "    output = np.zeros((N, D))\n",
        "    for offset in offsets:\n",
        "        shifted_center = centers + offset\n",
        "        new_center = np.repeat(shifted_center[None, :, :], N, axis=0)\n",
        "        output += np.array(np.linalg.norm(new_state - new_center, 2, axis=2) < widths, dtype=np.float32)\n",
        "\n",
        "    return output / len(offsets)\n",
        "\n",
        "def aggregation_features(state, centers):\n",
        "    state = torch.tensor(state, device=device)\n",
        "    centers = torch.tensor(centers, device=device)\n",
        "\n",
        "    distance = torch.sum((state[:, None, :] - centers[None, :, :])**2, dim=-1)\n",
        "    return (distance == distance.min(-1, keepdims=True).values) * 1.0  # make it float\n"
      ],
      "metadata": {
        "id": "MHAAP-0zM7YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = np.array([[1, 2]])\n",
        "c = np.array([[0, 0], [1, 1], [4, 4], [6, 6]])\n",
        "aggregation_features(s, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGcb4NywalLn",
        "outputId": "f74b515c-b535-4255-89bf-30c1d08e5616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5  1 13 41]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WZeWiE8IG57",
        "outputId": "4710b8b3-6e22-4d67-ac8e-39d875a30a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Define the neural network model\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        self.feature_size = self.feature_extract_init(state_size, action_size)\n",
        "        self.fc1 = nn.Linear(self.feature_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, action_size)\n",
        "        #self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        #self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "        self.to(device)\n",
        "\n",
        "    def feature_extract_init(self, state_size, action_size):\n",
        "        n_centers = [3, 3]\n",
        "        centers = np.array(\n",
        "          np.meshgrid(*[\n",
        "              np.linspace(env.observation_space.low[i], env.observation_space.high[i], n_centers[i])\n",
        "              for i in range(env.observation_space.shape[0])\n",
        "          ])\n",
        "        ).reshape(env.observation_space.shape[0], -1).T\n",
        "        centers = torch.tensor(centers).float().to(device)\n",
        "        self.feature_name, self.feature_extract = \"Aggregate\", lambda state : aggregation_features(state.reshape(-1, state_size), centers)\n",
        "        return self.feature_extract(env.reset()[0]).shape[1]\n",
        "\n",
        "    def forward(self, state):\n",
        "        #x = F.relu(self.fc1(state))\n",
        "        #x = F.relu(self.fc2(x))\n",
        "        #return self.fc3(x)\n",
        "        x = self.feature_extract(state)\n",
        "        return self.fc2(self.fc1(x))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMqPt_ruIG59"
      },
      "outputs": [],
      "source": [
        "# Define the DQN agent class\n",
        "class DQNAgent:\n",
        "    # Initialize the DQN agent\n",
        "    def __init__(self, state_size, action_size, seed, lr):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > 64:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, gamma=0.99)\n",
        "\n",
        "    # Choose an action based on the current state\n",
        "    def act(self, state, eps=0.):\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state_tensor)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if np.random.random() > eps:\n",
        "            return action_values.argmax(dim=1).item()\n",
        "        else:\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "    # Learn from batch of experiences\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        #print(loss)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sparisi/gym_gridworlds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7wvkhFJNdEe",
        "outputId": "49ba9fea-1e12-485f-f2d9-4a6a03d80b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gym_gridworlds'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 140 (delta 36), reused 51 (delta 24), pack-reused 69 (from 1)\u001b[K\n",
            "Receiving objects: 100% (140/140), 74.35 KiB | 1.91 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ./gym_gridworlds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ng8fN-lUmpi",
        "outputId": "e4be6e50-ff77-4791-8ef2-97cddc753df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/gym_gridworlds\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gymnasium (from Gym-Gridworlds==1.0)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from Gym-Gridworlds==1.0) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->Gym-Gridworlds==1.0) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium->Gym-Gridworlds==1.0)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, Gym-Gridworlds\n",
            "  Running setup.py develop for Gym-Gridworlds\n",
            "Successfully installed Gym-Gridworlds-1.0 farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.envs.registration import register\n",
        "\n",
        "register(\n",
        "    id=\"Gym-Gridworlds/Empty-2x2-v0\",\n",
        "    entry_point=\"gym_gridworlds.gridworld:GridworldRandomStart\",\n",
        "    max_episode_steps=10,\n",
        "    kwargs={\n",
        "        \"grid\": \"2x2_empty\",\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "x0vws2VIYSxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSD8eiX5IG5-",
        "outputId": "29bac94d-e995-4be7-cc3a-d1b66e85eb26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-93-389ec4c10993>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  centers = torch.tensor(centers, device=device)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the environment and the agent\n",
        "import gym\n",
        "import gymnasium\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Set up the environment\n",
        "env_id = \"Gym-Gridworlds/Empty-3x3-v0\"\n",
        "env = gymnasium.make(env_id, coordinate_observation=True, random_action_prob=0.1, reward_noise_std=0.01)\n",
        "env_eval = gymnasium.make(env_id, coordinate_observation=True, max_episode_steps=10)  # 10 steps only for faster eval\n",
        "episodes_eval = 10  # max expected return will be 0.994\n",
        "\n",
        "# Define training parameters\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 10\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.2\n",
        "epsilon_decay_rate = 0.99\n",
        "gamma = 0.9\n",
        "lr = 1e-3\n",
        "buffer_size = 100\n",
        "buffer = deque(maxlen=buffer_size)\n",
        "batch_size = 32\n",
        "update_frequency = 100\n",
        "\n",
        "\n",
        "# Initialize the DQNAgent\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "new_agent = DQNAgent(input_dim, output_dim, seed=170715, lr = lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "6rpzjfY2IG5-",
        "outputId": "495d2ae6-f311-4af0-d39b-54e0642460cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-93-389ec4c10993>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(state, device=device)\n",
            "<ipython-input-93-389ec4c10993>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  centers = torch.tensor(centers, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100: Finished training with reward 41.778254822966424\n",
            "Episode 200: Finished training with reward 83.82525071859243\n",
            "Episode 300: Finished training with reward 82.87260599972302\n",
            "Episode 400: Finished training with reward 96.99196246927394\n",
            "Episode 500: Finished training with reward 95.01423278351403\n",
            "Episode 600: Finished training with reward 96.14027723552594\n",
            "Episode 700: Finished training with reward 95.75091964923547\n",
            "Episode 800: Finished training with reward 97.67935285506772\n",
            "Episode 900: Finished training with reward 96.82794593994151\n",
            "Episode 1000: Finished training with reward 95.58492690068314\n",
            "Episode 1100: Finished training with reward 98.98421601976912\n",
            "Episode 1200: Finished training with reward 98.17582726059109\n",
            "Episode 1300: Finished training with reward 97.60103537390421\n",
            "Episode 1400: Finished training with reward 99.35196498302626\n",
            "Episode 1500: Finished training with reward 97.69023967546062\n",
            "Episode 1600: Finished training with reward 97.60274362553264\n",
            "Episode 1700: Finished training with reward 98.94257580087756\n",
            "Episode 1800: Finished training with reward 98.22170446064158\n",
            "Episode 1900: Finished training with reward 95.13405549244301\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-61447e5ecf5a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_start\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepsilon_decay_rate\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_reset_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gym_gridworlds/gym_gridworlds/gridworld.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gym_gridworlds/gym_gridworlds/gridworld.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, seed, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             self.agent_pos = (\n\u001b[0;32m--> 656\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "ep_mean_reward = 0\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment\n",
        "    state, _ = env.reset()\n",
        "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
        "\n",
        "    ep_reward = 0\n",
        "    # Run one episode\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Choose and perform an action\n",
        "        action = new_agent.act(state, epsilon)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        #print(reward)\n",
        "        buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        #print(len(buffer), batch_size)\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = random.sample(buffer, batch_size)\n",
        "            # Update the agent's knowledge\n",
        "            new_agent.learn(batch, gamma)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        ep_reward += reward\n",
        "        # Check if the episode has ended\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    ep_mean_reward += ep_reward\n",
        "    if (episode + 1) % update_frequency == 0:\n",
        "        print(f\"Episode {episode + 1}: Finished training with reward {ep_mean_reward}\")\n",
        "        ep_mean_reward = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT3DPKDFIG5_",
        "outputId": "29d414ed-2924-4172-ec2a-6e6c398e5e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward over 100 test episodes: 178.75\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the agent's performance\n",
        "test_episodes = 100\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = new_agent.act(state, eps=0.)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "average_reward = sum(episode_rewards) / test_episodes\n",
        "print(f\"Average reward over {test_episodes} test episodes: {average_reward:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-tXrSnDIG5_"
      },
      "outputs": [],
      "source": [
        "# Visualize the agent's performance\n",
        "import time\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = new_agent.act(state, eps=0.)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    time.sleep(0.1)  # Add a delay to make the visualization easier to follow\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}